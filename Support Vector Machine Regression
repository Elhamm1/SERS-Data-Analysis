# Conventional SVR version of the pipeline (same style, dirs, filenames, metrics, and plots)
import os
import glob
import time
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from scipy import stats
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    mean_absolute_percentage_error,
    explained_variance_score,
    make_scorer
)
from sklearn.linear_model import LinearRegression
import joblib

# === Parameters ===
input_length = 1496
train_data_dir = "../TrainingData"
val_data_dir   = "../TestingData"
num_epochs = 100              # kept for style; will be overwritten by number of CV candidates
batch_size = 32               # unused (kept for comparability)
lr = 1e-4                     # unused (kept for comparability)
patience = 10                 # unused (kept for comparability)
factor = 0.5                  # unused (kept for comparability)

# SmoothL1/Huber beta to mirror your torch loss (for CV scoring & loss plots)
SMOOTHL1_BETA = 0.02

# === Reproducibility ===
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# === Cultivar labels (values = true Cys for that cultivar) ===
cultivar_cys_map = {
    "AAC_Chrome":    0.3208,
    "AAC_Lacombe":   0.3428,
    "AAC_Liscard":   0.3181,
    "CDC_Amarillo":  0.3834,
    "CDC_Athabasca": 0.3418,
    "CDC_Canary":    0.3117,
    "CDC_Dakota":    0.3293,
    "CDC_Golden":    0.3536,
    "CDC_Greenwater":0.3446,
    "CDC_Inca":      0.3730,
    "CDC_Jasper":    0.3360,
    "CDC_Striker":   0.3575,
    "CDC_Lewochko":  0.3469,
    "CDC_Meadow":    0.3180,
    "CDC_Patrick":   0.3626,
    "CDC_Saffron":   0.3552,
    "CDC_Spectrum":  0.3948,
    "CDC_Spruce":    0.3535,
    "CDC_Tetris":    0.3531,
    "Redbat88":      0.3194,
}

# Optional SDs for later
cultivar_sd_map = {
    "AAC_Chrome":    0.0146,
    "AAC_Lacombe":   0.0401,
    "AAC_Liscard":   0.0601,
    "CDC_Amarillo":  0.0458,
    "CDC_Athabasca": 0.0215,
    "CDC_Canary":    0.0090,
    "CDC_Dakota":    0.0434,
    "CDC_Golden":    0.0425,
    "CDC_Greenwater":0.0086,
    "CDC_Inca":      0.0266,
    "CDC_Jasper":    0.0099,
    "CDC_Striker":   0.0182,
    "CDC_Lewochko":  0.0539,
    "CDC_Meadow":    0.0099,
    "CDC_Patrick":   0.0459,
    "CDC_Saffron":   0.0319,
    "CDC_Spectrum":  0.0275,
    "CDC_Spruce":    0.0028,
    "CDC_Tetris":    0.0034,
    "Redbat88":      0.0324,
}

# === Helpers to load files by cultivar prefix ===
def find_cultivar_from_filename(filename: str, cultivar_keys):
    """Return cultivar key if filename STARTS WITH that key, else None."""
    for key in cultivar_keys:
        if filename.startswith(key):
            return key
    return None

def load_dir(dir_path: str, cultivar_cys_map: dict, input_length: int):
    """
    Load all *.npy files from dir_path whose filenames start with a known cultivar.
    Returns (X, y):
      - X: np.ndarray of shape (N, input_length)
      - y: np.ndarray of shape (N,)
    Accepts arrays shaped (L,), (N, L), or (N, 1, L).
    """
    X_list, y_list = [], []
    cultivar_keys = list(cultivar_cys_map.keys())

    paths = sorted(glob.glob(os.path.join(dir_path, "*.npy")))
    if len(paths) == 0:
        print(f"âš ï¸ No .npy files found in {dir_path}")

    for file_path in paths:
        filename = os.path.basename(file_path)
        cultivar = find_cultivar_from_filename(filename, cultivar_keys)

        if cultivar is None:
            print(f"âš ï¸ Skipping unknown file (no cultivar prefix match): {filename}")
            continue

        cys_value = cultivar_cys_map[cultivar]
        arr = np.load(file_path)
        arr = np.asarray(arr)

        # Standardize shapes
        if arr.ndim == 1:
            # (L,) -> (1, L)
            arr = arr[None, :]
        elif arr.ndim == 3 and arr.shape[1] == 1:
            # (N, 1, L) -> (N, L)
            arr = arr[:, 0, :]
        elif arr.ndim != 2:
            raise ValueError(f"Unexpected array shape {arr.shape} in {filename}")

        if arr.shape[1] != input_length:
            raise ValueError(
                f"File {filename} has length {arr.shape[1]} != expected {input_length}"
            )

        X_list.append(arr)
        y_list.append(np.full((arr.shape[0],), cys_value, dtype=float))

    if len(X_list) == 0:
        raise RuntimeError(
            f"No usable files loaded from {dir_path}. "
            f"Check cultivar prefixes and maps."
        )

    X = np.vstack(X_list)
    y = np.hstack(y_list)
    return X, y

# === Load Data ===
# Train = everything in ../TrainingData; Test = everything in ../TestingData
X_train, Y_train = load_dir(train_data_dir, cultivar_cys_map, input_length)
X_val,   Y_val   = load_dir(val_data_dir,   cultivar_cys_map, input_length)

# === Utility: SmoothL1 (Huber) to mirror your torch loss for reporting/plots ===
def smooth_l1_loss(y_true, y_pred, beta=SMOOTHL1_BETA):
    diff = np.abs(y_true - y_pred)
    loss = np.where(diff < beta, 0.5 * (diff ** 2) / beta, diff - 0.5 * beta)
    return float(np.mean(loss))

def smooth_l1_loss_vec(y_true, y_pred):
    # vectorized scorer wrapper (signature for make_scorer)
    return -smooth_l1_loss(y_true, y_pred)  # negative because GridSearchCV maximizes the score

scorer = make_scorer(smooth_l1_loss_vec, greater_is_better=True)

# === Conventional SVR: scale + GridSearchCV ===
pipe = Pipeline([
    ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ("svr", SVR(kernel="rbf", cache_size=1000, tol=1e-3, max_iter=-1))
])

param_grid = {
    "svr__C":       [0.5, 1.0, 2.0, 5.0],
    "svr__epsilon": [0.01, 0.02, 0.05],
    "svr__gamma":   ["scale", 0.01],
}

cv = KFold(n_splits=5, shuffle=True, random_state=SEED)

start_time = time.time()
search = GridSearchCV(
    pipe,
    param_grid,
    scoring=scorer,           # maximizes (-SmoothL1) => minimizes SmoothL1
    cv=cv,
    n_jobs=-1,
    return_train_score=True,
    verbose=0
)
search.fit(X_train, Y_train)
elapsed = time.time() - start_time
print(f"â±ï¸ Total run time: {elapsed:.2f} seconds")
print(f"Best params -> {search.best_params_}")

best_model = search.best_estimator_
joblib.dump(best_model, "best_model.pkl")

# Build "loss curves" from CV results so Model_Loss.png is still produced
results = pd.DataFrame(search.cv_results_).sort_index()
train_losses = (-results["mean_train_score"]).tolist()
val_losses   = (-results["mean_test_score"]).tolist()
num_epochs = len(results)  # for consistent console style below

# Optional: print per-candidate like epochs, to keep your console style
for i, row in results.iterrows():
    params = row["params"]
    print(f"Epoch {i+1}/{num_epochs} | "
          f"Train Loss: {-row['mean_train_score']:.6f} | "
          f"Test Loss: {-row['mean_test_score']:.6f} | "
          f"Params: C={params['svr__C']}, eps={params['svr__epsilon']}, gamma={params['svr__gamma']}")

# === Evaluation ===
best_model = joblib.load("best_model.pkl")
preds   = best_model.predict(X_val)
targets = Y_val.copy()

# === Indexed Prediction Plot with Abs Error Bars ===
plt.figure(figsize=(10, 5))
indices = np.arange(len(preds))
errors = np.abs(preds - targets)
plt.errorbar(indices, preds, yerr=errors, fmt='o', alpha=0.7, capsize=3,
             label='Prediction Â± Abs Error')
plt.axhline(np.mean(targets), linestyle='--', label=f"Mean True Cys = {np.mean(targets):.3f}")
plt.xlabel("Spectrum Index")
plt.ylabel("Predicted Cys (g/100g)")
plt.title("Predictions on Testing Set with Absolute Error Bars")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("indexed_prediction_with_errorbars.png")
plt.close()

# === If the test set is a single cultivar, compute CI + histogram ===
unique_targets = np.unique(targets)
if len(unique_targets) == 1:
    mean_pred = np.mean(preds)
    std_pred = np.std(preds, ddof=1)
    n = len(preds)

    confidence_level = 0.90
    alpha = 1 - confidence_level
    t_critical = stats.t.ppf(1 - alpha/2, df=n-1)
    margin = t_critical * std_pred / np.sqrt(n)
    ci_lower, ci_upper = mean_pred - margin, mean_pred + margin

    true_cys = unique_targets[0]
    cultivar_label = [k for k, v in cultivar_cys_map.items() if abs(v - true_cys) < 1e-6]
    cultivar_name = cultivar_label[0] if cultivar_label else "Unknown"

    print(f"\nðŸ“Š Mean Prediction (Cultivar {cultivar_name})")
    print(f"Mean: {mean_pred:.6f}")
    print(f"{int(confidence_level*100)}% CI: [{ci_lower:.6f}, {ci_upper:.6f}] (Â± {margin:.6f})")

    # Error bar plot with CI
    plt.figure()
    plt.errorbar(x=[true_cys], y=[mean_pred],
                 yerr=[[mean_pred - ci_lower], [ci_upper - mean_pred]],
                 fmt='o', capsize=5, label=f'Mean Â± {int(confidence_level*100)}% CI')
    plt.axline((true_cys, true_cys), slope=1, linestyle='--', label='Ideal')
    plt.xlabel("True Cys (g/100g)")
    plt.ylabel("Predicted Mean Cys (g/100g)")
    plt.title(f"Mean Prediction with {int(confidence_level*100)}% CI (n={n})")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig("mean_prediction_with_CI_singlecultivar.png")
    plt.close()

    # Histogram of predictions
    plt.figure()
    plt.hist(preds, bins=20, edgecolor='black')
    plt.axvline(mean_pred, linestyle='--', label=f'Mean: {mean_pred:.3f}')
    plt.axvline(ci_lower, linestyle=':', label=f'{int(confidence_level*100)}% CI')
    plt.axvline(ci_upper, linestyle=':')
    plt.title("Distribution of Predictions (Single Cultivar)")
    plt.xlabel("Predicted Cys (g/100g)")
    plt.ylabel("Frequency")
    plt.legend()
    plt.tight_layout()
    plt.savefig("prediction_distribution_singlecultivar.png")
    plt.close()
else:
    print(f"Skipping CI & histogram: test set has {len(targets)} spectra "
          f"or multiple cultivars ({len(unique_targets)} unique Cys values).")

# === Metrics ===
mae  = mean_absolute_error(targets, preds)
rmse = np.sqrt(mean_squared_error(targets, preds))
r2   = r2_score(targets, preds)

print("\nðŸ“ˆ Final Model Performance (Testing Set):")
print(f"MAE                 : {mae:.6f}")
print(f"RMSE                : {rmse:.6f}")
print(f"RÂ²                  : {r2:.6f}")
print(f"Max Error           : {max_error(targets, preds):{'.6f'}}")
print(f"MAPE                : {mean_absolute_percentage_error(targets, preds):.6f}")
print(f"Explained Variance  : {explained_variance_score(targets, preds):.6f}")

# === Create a small results table (optional) ===
cys_to_cultivar = {round(v, 3): k for k, v in cultivar_cys_map.items()}
df = pd.DataFrame({"True Cys": targets, "Predicted Cys": preds})
df["Cultivar"] = df["True Cys"].apply(lambda x: cys_to_cultivar.get(round(x, 3), "Unknown"))
df.to_csv("test_predictions.csv", index=False)

# === Separate Error Bars (Model vs Measurement) ===
plt.figure(figsize=(8, 6))

seen = set()  # avoid duplicate cultivar labels
for cultivar in df["Cultivar"].unique():
    sub = df[df["Cultivar"] == cultivar]
    y_true = sub["True Cys"].to_numpy()
    y_pred = sub["Predicted Cys"].to_numpy()

    # Vertical = model absolute error per spectrum; Horizontal = HPLC SD
    model_error = np.abs(y_pred - y_true)
    meas_error  = float(cultivar_sd_map.get(cultivar, 0.0))

    label = cultivar if cultivar not in seen else "_nolegend_"
    seen.add(cultivar)

    # vertical (model) error bars
    plt.errorbar(y_true, y_pred, yerr=model_error,
                 fmt='o', label=label, alpha=0.8, capsize=3, elinewidth=1.2)

    # horizontal (measurement) error bars
    plt.errorbar(y_true, y_pred, xerr=meas_error,
                 fmt='none', ecolor='gray', linestyle='dotted',
                 capsize=2, elinewidth=1.0)

# Ideal diagonal (no legend entry)
mn = float(min(df["True Cys"].min(), df["Predicted Cys"].min()))
mx = float(max(df["True Cys"].max(), df["Predicted Cys"].max()))
plt.plot([mn, mx], [mn, mx], 'k--', label="_nolegend_")

plt.title("Predicted vs. True with Separate Error Bars (Model vs. Measurement)")
plt.xlabel("True Cys (g/100g)")
plt.ylabel("Predicted Cys (g/100g)")
plt.grid(True, alpha=0.2)

# Cultivar legend INSIDE the plot
plt.legend(loc='upper left', frameon=True, ncol=1)
plt.tight_layout()
plt.savefig("Predicted_vs_True_Separate_Errors.png")
plt.close()

# === Separate Error Bars (MEAN per cultivar) ===
g = (df.groupby("Cultivar")
       .agg(True_Cys=("True Cys", "first"),          # constant per cultivar
            Mean_Pred=("Predicted Cys", "mean"),     # mean prediction
            Std_Pred=("Predicted Cys", "std"),       # SD across predictions
            N=("Predicted Cys", "size"))             # number of spectra
       .reset_index())

plt.figure(figsize=(10, 6))

# Choose whether vertical error bars show SD (spread) or SE (uncertainty of mean)
use_standard_error = False  # True => SD/sqrt(N); False => SD

for _, row in g.iterrows():
    cultivar = row["Cultivar"]
    x = float(row["True_Cys"])
    y = float(row["Mean_Pred"])
    # vertical error bar: SD or SE
    yerr = float(row["Std_Pred"] / np.sqrt(row["N"]) if use_standard_error else row["Std_Pred"])
    # horizontal error bar: HPLC SD for this cultivar (0.0 if missing)
    xerr = float(cultivar_sd_map.get(cultivar, 0.0))

    # Plot the point + vertical error
    plt.errorbar(x, y, yerr=yerr, fmt='o', capsize=3, elinewidth=1.2,
                 label=cultivar, alpha=0.9)

    # Add horizontal error (measurement) as dotted gray bars (disabled)
    plt.errorbar(x, y, xerr=xerr, fmt='none', ecolor='gray',
                 linestyle='dotted', capsize=2, elinewidth=1.0)

# --- Common range from data (compute ONCE) ---
mn = float(min(g["True_Cys"].min(), g["Mean_Pred"].min()))
mx = float(max(g["True_Cys"].max(), g["Mean_Pred"].max()))

# --- Make the axes square and set limits BEFORE drawing lines ---
ax = plt.gca()
ax.set_aspect('equal', adjustable='box')   # identity line appears at 45Â°
pad = 0.002                                # small margin so bars don't touch frame
ax.set_xlim(mn - pad, mx + pad)
ax.set_ylim(mn - pad, mx + pad)

# --- Identity line (draw ONCE, after limits) ---
ax.plot([mn - pad, mx + pad], [mn - pad, mx + pad],
        'k--', label='Ideal (y=x)')

# --- Fit a simple OLS line: Mean_Pred ~ True_Cys ---
X_fit = g["True_Cys"].to_numpy().reshape(-1, 1)
y_fit = g["Mean_Pred"].to_numpy()
reg = LinearRegression().fit(X_fit, y_fit)
slope = float(reg.coef_[0])
intercept = float(reg.intercept_)
x_line = np.linspace(mn - pad, mx + pad, 200)
ax.plot(x_line, slope * x_line + intercept, '-', linewidth=2, color='C1',
        label=f'Fit: y={slope:.3f}x+{intercept:.3f}')

# --- RÂ² values ---
r2_means = r2_score(g["True_Cys"], g["Mean_Pred"])
r2_fit   = r2_score(y_fit, reg.predict(X_fit))

# Annotate both RÂ² values inside the plot (top-left)
y_std   = targets.std(ddof=1)
y_range = targets.max() - targets.min()
rmse    = np.sqrt(mean_squared_error(targets, preds))

ax.text(0.02, 0.98, f"RÂ² vs identity = {r2_means:.3f}\nRÂ² of fit = {r2_fit:.3f}",
        transform=ax.transAxes, ha="left", va="top",
        bbox=dict(boxstyle="round", facecolor="white", alpha=0.7))

# Add normalized error annotation (uses rmse, targets computed earlier)
ax.text(
    0.02, 0.82,
    f"RMSE/std(y) = {rmse / y_std:.3f}\nRMSE/range(y) = {rmse / y_range:.3f}",
    transform=ax.transAxes, ha="left", va="top",
    bbox=dict(boxstyle="round", facecolor="white", alpha=0.7)
)

# Final formatting + save
plt.title("Mean Predicted vs. True HPLC per Cultivar")
plt.xlabel("True HPLC Cys (g/100g)")
plt.ylabel("Mean Predicted Cys (g/100g)")
plt.grid(False, alpha=0.2)
# Legend outside on the right (keep same figure size)
ax = plt.gca()
ax.legend(
    loc="upper left",
    bbox_to_anchor=(1.02, 1.0),   # just outside the axes
    borderaxespad=0.0,
    frameon=True
)

# Leave room on the right so the legend isn't clipped
plt.tight_layout(rect=(0.0, 0.0, 0.80, 1.0))
plt.savefig("Predicted_vs_True_Separate_Errors_MEAN.png")
plt.close()

# --- Optional: print mean-level metrics AFTER the figure is closed ---
mae_means  = mean_absolute_error(g["True_Cys"], g["Mean_Pred"])
rmse_means = np.sqrt(mean_squared_error(g["True_Cys"], g["Mean_Pred"]))
print("\nPer-cultivar mean metrics:")
print(f"RÂ²(means) : {r2_means:.6f}")   # same as 'vs identity' shown on plot
print(f"MAE(means): {mae_means:.6f}")
print(f"RMSE(means): {rmse_means:.6f}")

# Per-spectrum normalized error (uses `rmse`, `targets` computed earlier)
print("\nNormalized error (per-spectrum):")
print(f"RMSE / std(y):   {rmse / y_std:.3f}")
print(f"RMSE / range(y): {rmse / y_range:.3f}")

# === Predicted vs True scatter for the test set ===
plt.figure(figsize=(6, 6))
plt.scatter(targets, preds, alpha=0.7)
mn_scatter, mx_scatter = min(targets.min(), preds.min()), max(targets.max(), preds.max())
plt.plot([mn_scatter, mx_scatter], [mn_scatter, mx_scatter], 'k--', label='Ideal')
plt.xlabel("True Cys (g/100g)")
plt.ylabel("Predicted Cys (g/100g)")
plt.title("Predicted vs True Cys (Testing Set)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("calibration_plot_testing_set.png")
plt.close()

# === Loss curves (from CV results, to mirror your file/plot names) ===
plt.figure()
plt.plot(train_losses, label="Train")
plt.plot(val_losses, label="Validation")
plt.title("Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("SmoothL1 Loss")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.savefig("Model_Loss.png")
plt.close()

# === Bar plot: mean Predicted vs True (per cultivar) ===
# Reuse 'g' from above: columns = ["Cultivar", "True_Cys", "Mean_Pred", "Std_Pred", "N"]
if 'use_standard_error' not in globals():
    use_standard_error = False  # default if not already set

yerr_pred = (g["Std_Pred"] / np.sqrt(g["N"])) if use_standard_error else g["Std_Pred"]
yerr_pred = np.nan_to_num(yerr_pred.to_numpy(), nan=0.0)  # handle N=1 edge case

# Optional: show HPLC measurement SD on the "True" bars
SHOW_HPLC_SD = False
yerr_true = np.array([cultivar_sd_map.get(cv, 0.0) for cv in g["Cultivar"]]) if SHOW_HPLC_SD else None

plt.figure(figsize=(10, 6))
x = np.arange(len(g))
bar_w = 0.38

# True (HPLC) bars
plt.bar(x - bar_w/2, g["True_Cys"], width=bar_w, label="True (HPLC)",
        yerr=yerr_true, capsize=3 if SHOW_HPLC_SD else 0)

# Predicted (mean) bars
plt.bar(x + bar_w/2, g["Mean_Pred"], width=bar_w, label=f"Predicted (mean){' Â± SE' if use_standard_error else ' Â± SD'}",
        yerr=yerr_pred, capsize=3)

plt.xticks(x, g["Cultivar"], rotation=45, ha="right")
plt.ylabel("Cys (g/100g)")
plt.title("Mean Predicted vs True (HPLC) per Cultivar")
plt.grid(axis='y', alpha=0.2)
plt.legend()
plt.tight_layout()
plt.savefig("bar_mean_true_vs_pred.png")
plt.close()

# === Save per-cultivar table with absolute errors ===
g_out = g.copy()
g_out["Abs_Error"]    = (g_out["Mean_Pred"] - g_out["True_Cys"]).abs()
g_out["Pct_Error_%"]  = 100.0 * g_out["Abs_Error"] / g_out["True_Cys"]
g_out["SE_Pred"]      = g_out["Std_Pred"] / np.sqrt(g_out["N"])

# Column order for readability
cols = ["Cultivar", "True_Cys", "Mean_Pred", "Abs_Error", "Pct_Error_%", "N", "Std_Pred", "SE_Pred"]
g_out[cols].to_csv("mean_vs_true_by_cultivar.csv", index=False)

print("\nSaved bar plot -> bar_mean_true_vs_pred.png")
print("Saved per-cultivar summary -> mean_vs_true_by_cultivar.csv")
