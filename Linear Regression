import os, glob, json, time
import numpy as np
import pandas as pd
from typing import Dict

from sklearn.linear_model import LinearRegression
from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    max_error,
    explained_variance_score,
)

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D

# ============================
# Parameters & Directories
# ============================

input_length = 1496
train_data_dir = "../TrainingData"
val_data_dir   = "../TestingData"

# === Cultivar labels (values = true Cys for that cultivar) ===
cultivar_cys_map = {
    "AAC_Chrome":    0.3208,
    "AAC_Lacombe":   0.3428,
    "AAC_Liscard":   0.3181,
    "CDC_Amarillo":  0.3834,
    "CDC_Athabasca": 0.3418,
    "CDC_Canary":    0.3117,
    "CDC_Dakota":    0.3293,
    "CDC_Golden":    0.3536,
    "CDC_Greenwater":0.3446,
    "CDC_Inca":      0.3730,
    "CDC_Jasper":    0.3360,
    "CDC_Striker":   0.3575,
    "CDC_Lewochko":  0.3469,
    "CDC_Meadow":    0.3180,
    "CDC_Patrick":   0.3626,
    "CDC_Saffron":   0.3552,
    "CDC_Spectrum":  0.3948,
    "CDC_Spruce":    0.3535,
    "CDC_Tetris":    0.3531,
    "Redbat88":      0.3194,
}

# Optional SDs for later (HPLC measurement SDs)
cultivar_sd_map = {
    "AAC_Chrome":    0.0146,
    "AAC_Lacombe":   0.0401,
    "AAC_Liscard":   0.0601,
    "CDC_Amarillo":  0.0458,
    "CDC_Athabasca": 0.0215,
    "CDC_Canary":    0.0090,
    "CDC_Dakota":    0.0434,
    "CDC_Golden":    0.0425,
    "CDC_Greenwater":0.0086,
    "CDC_Inca":      0.0266,
    "CDC_Jasper":    0.0099,
    "CDC_Striker":   0.0182,
    "CDC_Lewochko":  0.0539,
    "CDC_Meadow":    0.0099,
    "CDC_Patrick":   0.0459,
    "CDC_Saffron":   0.0319,
    "CDC_Spectrum":  0.0275,
    "CDC_Spruce":    0.0028,
    "CDC_Tetris":    0.0034,
    "Redbat88":      0.0324,
}

# Output files prefix (kept)
out_prefix = "ols_linear_regression"

# ============================
# Helpers to load files by cultivar prefix (MATCHES CNN)
# ============================

def find_cultivar_from_filename(filename: str, cultivar_keys):
    """Return cultivar key if filename STARTS WITH that key, else None."""
    for key in cultivar_keys:
        if filename.startswith(key):
            return key
    return None

def load_dir(dir_path: str, cultivar_cys_map: dict, input_length: int):
    """
    Load all *.npy files from dir_path whose filenames start with a known cultivar.
    Returns (X, y):
      - X: np.ndarray of shape (N, input_length)
      - y: np.ndarray of shape (N,)
    Accepts arrays shaped (L,), (N, L), or (N, 1, L).
    """
    X_list, y_list = [], []
    cultivar_keys = list(cultivar_cys_map.keys())

    paths = sorted(glob.glob(os.path.join(dir_path, "*.npy")))
    if len(paths) == 0:
        print(f"‚ö†Ô∏è No .npy files found in {dir_path}")

    for file_path in paths:
        filename = os.path.basename(file_path)
        cultivar = find_cultivar_from_filename(filename, cultivar_keys)

        if cultivar is None:
            # silent skip to mirror CNN's behavior (printed warning there)
            # keep a message for visibility
            print(f"‚ö†Ô∏è Skipping unknown file (no cultivar prefix match): {filename}")
            continue

        cys_value = cultivar_cys_map[cultivar]
        arr = np.load(file_path)
        arr = np.asarray(arr)

        # Standardize shapes
        if arr.ndim == 1:
            # (L,) -> (1, L)
            arr = arr[None, :]
        elif arr.ndim == 3 and arr.shape[1] == 1:
            # (N, 1, L) -> (N, L)
            arr = arr[:, 0, :]
        elif arr.ndim != 2:
            raise ValueError(f"Unexpected array shape {arr.shape} in {filename}")

        if arr.shape[1] != input_length:
            raise ValueError(
                f"File {filename} has length {arr.shape[1]} != expected {input_length}"
            )

        X_list.append(arr)
        y_list.append(np.full((arr.shape[0],), cys_value, dtype=float))

    if len(X_list) == 0:
        raise RuntimeError(
            f"No usable files loaded from {dir_path}. "
            f"Check cultivar prefixes and maps."
        )

    X = np.vstack(X_list)
    y = np.hstack(y_list)
    return X, y

# ============================
# Data loading 
# ============================

start_time = time.time()

X_train, Y_train = load_dir(train_data_dir, cultivar_cys_map, input_length)
X_val,   Y_val   = load_dir(val_data_dir,   cultivar_cys_map, input_length)

# ============================
# Fit OLS
# ============================

model = LinearRegression()  # fit_intercept=True by default
model.fit(X_train, Y_train)

ytr_pred = model.predict(X_train)
yva_pred = model.predict(X_val)

# ============================
# Metrics 
# ============================

def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:
    eps = 1e-12
    y_true = y_true.astype(float).ravel()
    y_pred = y_pred.astype(float).ravel()
    mse  = float(mean_squared_error(y_true, y_pred))
    rmse = float(np.sqrt(mse))
    mae  = float(mean_absolute_error(y_true, y_pred))
    r2   = float(r2_score(y_true, y_pred))
    evs  = float(explained_variance_score(y_true, y_pred))
    mxe  = float(max_error(y_true, y_pred))
    if y_true.std() < eps or y_pred.std() < eps:
        r = float("nan")
    else:
        r = float(np.corrcoef(y_true, y_pred)[0, 1])
    bias = float(np.mean(y_pred - y_true))
    mape = float(np.mean(np.abs((y_true - y_pred) / (y_true + eps)))) * 100.0
    return dict(MSE=mse, RMSE=rmse, MAE=mae, R2=r2, Pearson_r=r, Bias=bias,
                MAPE_pct=mape, ExplainedVar=evs, MaxError=mxe)

train_metrics = compute_metrics(Y_train, ytr_pred)
val_metrics   = compute_metrics(Y_val,   yva_pred)

metrics = {
    "train": train_metrics,
    "val":   val_metrics,
    "in_dim": int(X_train.shape[1]),
    "estimator": "LinearRegression(fit_intercept=True)"
}
with open(f"{out_prefix}_metrics.json", "w") as f:
    json.dump(metrics, f, indent=2)

# Console-style summary (mirrors CNN style)
print("\nüìà Final Model Performance (Testing/Validation Set):")
print(f"MAE                 : {val_metrics['MAE']:.6f}")
print(f"RMSE                : {val_metrics['RMSE']:.6f}")
print(f"R¬≤                  : {val_metrics['R2']:.6f}")
print(f"Max Error           : {val_metrics['MaxError']:.6f}")
print(f"MAPE (%)            : {val_metrics['MAPE_pct']:.6f}")
print(f"Explained Variance  : {val_metrics['ExplainedVar']:.6f}")
print(f"‚è±Ô∏è Total run time: {time.time() - start_time:.2f} seconds")
# ============================
# Artifacts: Plots & CSVs (mirroring CNN outputs)
# ============================

# --- Common helpers
cys_to_cultivar = {round(v, 3): k for k, v in cultivar_cys_map.items()}

# Val dataframe (like CNN's test_predictions.csv logic)
df_val = pd.DataFrame({"True Cys": Y_val, "Predicted Cys": yva_pred})
df_val["Cultivar"] = df_val["True Cys"].apply(lambda x: cys_to_cultivar.get(round(x, 3), "Unknown"))
df_val.to_csv("test_predictions.csv", index=False)  # same filename as CNN

# 1) Indexed Prediction Plot with Absolute Error Bars (VALIDATION)
plt.figure(figsize=(10, 5))
indices = np.arange(len(yva_pred))
errors = np.abs(yva_pred - Y_val)
plt.errorbar(indices, yva_pred, yerr=errors, fmt='o', alpha=0.7, capsize=3,
             label='Prediction ¬± Abs Error')
plt.axhline(np.mean(Y_val), linestyle='--', label=f"Mean True Cys = {np.mean(Y_val):.3f}")
plt.xlabel("Spectrum Index")
plt.ylabel("Predicted Cys (g/100g)")
plt.title("Predictions on Testing/Validation Set with Absolute Error Bars")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("indexed_prediction_with_errorbars.png")
plt.close()

# 2) If the test set is a single cultivar, compute CI + histogram (VALIDATION)
unique_targets = np.unique(Y_val)
if len(unique_targets) == 1:
    try:
        from scipy import stats
        mean_pred = float(np.mean(yva_pred))
        std_pred  = float(np.std(yva_pred, ddof=1))
        n = int(len(yva_pred))
        confidence_level = 0.90
        alpha = 1 - confidence_level
        t_critical = float(stats.t.ppf(1 - alpha/2, df=n-1))
        margin = t_critical * std_pred / np.sqrt(max(n, 1))
        ci_lower, ci_upper = mean_pred - margin, mean_pred + margin

        true_cys = float(unique_targets[0])
        cultivar_label = [k for k, v in cultivar_cys_map.items() if abs(v - true_cys) < 1e-6]
        cultivar_name = cultivar_label[0] if cultivar_label else "Unknown"

        print(f"\nüìä Mean Prediction (Cultivar {cultivar_name})")
        print(f"Mean: {mean_pred:.6f}")
        print(f"{int(confidence_level*100)}% CI: [{ci_lower:.6f}, {ci_upper:.6f}] (¬± {margin:.6f})")

        # Error bar plot with CI
        plt.figure()
        plt.errorbar(x=[true_cys], y=[mean_pred],
                     yerr=[[mean_pred - ci_lower], [ci_upper - mean_pred]],
                     fmt='o', capsize=5, label=f'Mean ¬± {int(confidence_level*100)}% CI')
        # identity
        plt.axline((true_cys, true_cys), slope=1, linestyle='--', label='Ideal')
        plt.xlabel("True Cys (g/100g)")
        plt.ylabel("Predicted Mean Cys (g/100g)")
        plt.title(f"Mean Prediction with {int(confidence_level*100)}% CI (n={n})")
        plt.grid(True)
        plt.legend()
        plt.tight_layout()
        plt.savefig("mean_prediction_with_CI_singlecultivar.png")
        plt.close()

        # Histogram of predictions
        plt.figure()
        plt.hist(yva_pred, bins=20, edgecolor='black')
        plt.axvline(mean_pred, linestyle='--', label=f'Mean: {mean_pred:.3f}')
        plt.axvline(ci_lower, linestyle=':', label=f'{int(confidence_level*100)}% CI')
        plt.axvline(ci_upper, linestyle=':')
        plt.title("Distribution of Predictions (Single Cultivar)")
        plt.xlabel("Predicted Cys (g/100g)")
        plt.ylabel("Frequency")
        plt.legend()
        plt.tight_layout()
        plt.savefig("prediction_distribution_singlecultivar.png")
        plt.close()
    except Exception as e:
        print(f"‚ö†Ô∏è Skipping CI/hist (scipy not available or error): {e}")
else:
    print(f"Skipping CI & histogram: test set has {len(Y_val)} spectra "
          f"or multiple cultivars ({len(np.unique(df_val['Cultivar']))} unique Cys values).")

# 3) Separate Error Bars (Model vs Measurement) ‚Äî per-spectrum (VALIDATION)
plt.figure(figsize=(8, 6))

seen = set()  # avoid duplicate legend labels
for cultivar in df_val["Cultivar"].unique():
    sub = df_val[df_val["Cultivar"] == cultivar]
    y_true = sub["True Cys"].to_numpy()
    y_pred = sub["Predicted Cys"].to_numpy()

    model_error = np.abs(y_pred - y_true)                 # vertical
    meas_error  = float(cultivar_sd_map.get(cultivar, 0.0))  # horizontal

    label = cultivar if cultivar not in seen else "_nolegend_"
    seen.add(cultivar)

    # vertical (model) error bars
    plt.errorbar(y_true, y_pred, yerr=model_error,
                 fmt='o', label=label, alpha=0.8, capsize=3, elinewidth=1.2)

    # horizontal (measurement) error bars
    plt.errorbar(y_true, y_pred, xerr=meas_error,
                 fmt='none', ecolor='gray', linestyle='dotted',
                 capsize=2, elinewidth=1.0)

# Ideal diagonal
mn = float(min(df_val["True Cys"].min(), df_val["Predicted Cys"].min()))
mx = float(max(df_val["True Cys"].max(), df_val["Predicted Cys"].max()))
plt.plot([mn, mx], [mn, mx], 'k--', label="_nolegend_")

plt.title("Predicted vs. True with Separate Error Bars (Model vs. Measurement)")
plt.xlabel("True Cys (g/100g)")
plt.ylabel("Predicted Cys (g/100g)")
plt.grid(True, alpha=0.2)
plt.legend(loc='upper left', frameon=True, ncol=1)
plt.tight_layout()
plt.savefig("Predicted_vs_True_Separate_Errors.png")
plt.close()

# 4) Separate Error Bars (MEAN per cultivar) + OLS fit on means (VALIDATION)
g = (df_val.groupby("Cultivar")
       .agg(True_Cys=("True Cys", "first"),          # constant per cultivar
            Mean_Pred=("Predicted Cys", "mean"),     # mean prediction
            Std_Pred=("Predicted Cys", "std"),       # SD across predictions
            N=("Predicted Cys", "size"))             # number of spectra
       .reset_index())

plt.figure(figsize=(10, 6))

# Choose whether vertical error bars show SD (spread) or SE (uncertainty of mean)
use_standard_error = False  # True => SD/sqrt(N); False => SD

for _, row in g.iterrows():
    cultivar = row["Cultivar"]
    x = float(row["True_Cys"])
    y = float(row["Mean_Pred"])
    # vertical error bar: SD or SE
    yerr = float(row["Std_Pred"] / np.sqrt(row["N"]) if use_standard_error else row["Std_Pred"])
    # horizontal error bar: HPLC SD for this cultivar (0.0 if missing)
    xerr = float(cultivar_sd_map.get(cultivar, 0.0))

    # Plot the point + vertical error
    plt.errorbar(x, y, yerr=yerr, fmt='o', capsize=3, elinewidth=1.2,
                 label=cultivar, alpha=0.9)

    # Add horizontal error (measurement) as dotted gray bars
    plt.errorbar(x, y, xerr=xerr, fmt='none', ecolor='gray',
                 linestyle='dotted', capsize=2, elinewidth=1.0)

# --- Common range from data (compute ONCE) ---
mn = float(min(g["True_Cys"].min(), g["Mean_Pred"].min()))
mx = float(max(g["True_Cys"].max(), g["Mean_Pred"].max()))

# --- Make the axes square and set limits BEFORE drawing lines ---
ax = plt.gca()
ax.set_aspect('equal', adjustable='box')   # identity line appears at 45¬∞
pad = 0.002                                # small margin so bars don't touch frame
ax.set_xlim(mn - pad, mx + pad)
ax.set_ylim(mn - pad, mx + pad)

# --- Identity line (draw ONCE, after limits) ---
ax.plot([mn - pad, mx + pad], [mn - pad, mx + pad],
        'k--', label='Ideal (y=x)')

# --- Fit a simple OLS line: Mean_Pred ~ True_Cys ---
X_means = g["True_Cys"].to_numpy().reshape(-1, 1)
y_means = g["Mean_Pred"].to_numpy()
reg = LinearRegression().fit(X_means, y_means)
slope = float(reg.coef_[0])
intercept = float(reg.intercept_)
x_line = np.linspace(mn - pad, mx + pad, 200)
ax.plot(x_line, slope * x_line + intercept, '-', linewidth=2, color='C1',
        label=f'Fit: y={slope:.3f}x+{intercept:.3f}')

# --- R¬≤ values ---
# r2_means: compares your points to the identity line (how close to y=x overall)
# r2_fit  : R¬≤ of the best-fit line (how well a linear model explains the means)
r2_means = r2_score(g["True_Cys"], g["Mean_Pred"])
r2_fit   = r2_score(y_means, reg.predict(X_means))

# Annotate both R¬≤ values + normalized error (per-spectrum, from VALIDATION)
y_std   = Y_val.std(ddof=1)
y_range = Y_val.max() - Y_val.min()
rmse_val = float(val_metrics["RMSE"])

ax.text(0.02, 0.98, f"R¬≤ vs identity = {r2_means:.3f}\nR¬≤ of fit = {r2_fit:.3f}",
        transform=ax.transAxes, ha="left", va="top",
        bbox=dict(boxstyle="round", facecolor="white", alpha=0.7))

ax.text(
    0.02, 0.82,
    f"RMSE/std(y) = {rmse_val / y_std:.3f}\nRMSE/range(y) = {rmse_val / y_range:.3f}",
    transform=ax.transAxes, ha="left", va="top",
    bbox=dict(boxstyle="round", facecolor="white", alpha=0.7)
)

# Final formatting + save
plt.title("Mean Predicted vs. True HPLC per Cultivar")
plt.xlabel("True HPLC Cys (g/100g)")
plt.ylabel("Mean Predicted Cys (g/100g)")
plt.grid(False, alpha=0.2)
ax.legend(
    loc="upper left",
    bbox_to_anchor=(1.02, 1.0),   # just outside the axes
    borderaxespad=0.0,
    frameon=True
)
plt.tight_layout(rect=(0.0, 0.0, 0.80, 1.0))
plt.savefig("Predicted_vs_True_Separate_Errors_MEAN.png")
plt.close()

# --- Optional: print mean-level metrics AFTER the figure is closed ---
mae_means  = float(mean_absolute_error(g["True_Cys"], g["Mean_Pred"]))
rmse_means = float(np.sqrt(mean_squared_error(g["True_Cys"], g["Mean_Pred"])))
print("\nPer-cultivar mean metrics:")
print(f"R¬≤(means) : {r2_means:.6f}")   # same as 'vs identity' shown on plot
print(f"MAE(means): {mae_means:.6f}")
print(f"RMSE(means): {rmse_means:.6f}")

# Per-spectrum normalized error (uses `rmse_val`, `Y_val`)
print("\nNormalized error (per-spectrum):")
print(f"RMSE / std(y):   {rmse_val / y_std:.3f}")
print(f"RMSE / range(y): {rmse_val / y_range:.3f}")

# 5) Predicted vs True scatter for the validation set (calibration)
plt.figure(figsize=(6, 6))
plt.scatter(Y_val, yva_pred, alpha=0.7)
mn_sc, mx_sc = min(Y_val.min(), yva_pred.min()), max(Y_val.max(), yva_pred.max())
plt.plot([mn_sc, mx_sc], [mn_sc, mx_sc], 'k--', label='Ideal')
plt.xlabel("True Cys (g/100g)")
plt.ylabel("Predicted Cys (g/100g)")
plt.title("Predicted vs True Cys (Testing/Validation Set)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("calibration_plot_testing_set.png")
plt.close()

# 6) Bar plot: mean Predicted vs True (per cultivar) + CSV (VALIDATION)
g_out = g.copy()
g_out["Abs_Error"]    = (g_out["Mean_Pred"] - g_out["True_Cys"]).abs()
g_out["Pct_Error_%"]  = 100.0 * g_out["Abs_Error"] / g_out["True_Cys"]
g_out["SE_Pred"]      = g_out["Std_Pred"] / np.sqrt(g_out["N"])

# Column order for readability
cols = ["Cultivar", "True_Cys", "Mean_Pred", "Abs_Error", "Pct_Error_%", "N", "Std_Pred", "SE_Pred"]
g_out[cols].to_csv("mean_vs_true_by_cultivar.csv", index=False)

# Choose whether to show SD (spread) or SE (uncertainty of the mean) as error bars on the Predicted bars
if 'use_standard_error' not in globals():
    use_standard_error = False  # default if not already set

yerr_pred = (g["Std_Pred"] / np.sqrt(g["N"])) if use_standard_error else g["Std_Pred"]
yerr_pred = np.nan_to_num(yerr_pred.to_numpy(), nan=0.0)  # handle N=1 edge case

SHOW_HPLC_SD = False
yerr_true = np.array([cultivar_sd_map.get(cv, 0.0) for cv in g["Cultivar"]]) if SHOW_HPLC_SD else None

plt.figure(figsize=(10, 6))
x = np.arange(len(g))
bar_w = 0.38

# True (HPLC) bars
plt.bar(x - bar_w/2, g["True_Cys"], width=bar_w, label="True (HPLC)",
        yerr=yerr_true, capsize=3 if SHOW_HPLC_SD else 0)

# Predicted (mean) bars
plt.bar(x + bar_w/2, g["Mean_Pred"], width=bar_w, label=f"Predicted (mean){' ¬± SE' if use_standard_error else ' ¬± SD'}",
        yerr=yerr_pred, capsize=3)

plt.xticks(x, g["Cultivar"], rotation=45, ha="right")
plt.ylabel("Cys (g/100g)")
plt.title("Mean Predicted vs True (HPLC) per Cultivar")
plt.grid(axis='y', alpha=0.2)
plt.legend()
plt.tight_layout()
plt.savefig("bar_mean_true_vs_pred.png")
plt.close()

print("\nSaved bar plot -> bar_mean_true_vs_pred.png")
print("Saved per-cultivar summary -> mean_vs_true_by_cultivar.csv")

# 7) ‚ÄúLoss curve‚Äù stand-in for OLS: bar chart of Train vs Val MSE (named like CNN)
plt.figure()
plt.bar(["Train MSE", "Val MSE"], [train_metrics["MSE"], val_metrics["MSE"]])
plt.title("Loss (MSE) ‚Äî OLS (no epochs)")
plt.ylabel("MSE")
plt.tight_layout()
plt.savefig("Model_Loss.png")
plt.close()

# 8) Full predictions CSV (Train + Val) ‚Äî keep your existing artifact name too
full_pred = pd.DataFrame({
    "Split": np.array(["Train"] * len(ytr_pred) + ["Val"] * len(yva_pred)),
    "Cultivar": np.concatenate([
        np.array(["Train"] * len(ytr_pred)),
        df_val["Cultivar"].values
    ]),
    "True_Cys": np.concatenate([Y_train, Y_val]),
    "Pred_Cys": np.concatenate([ytr_pred, yva_pred])
})
full_pred.to_csv(f"{out_prefix}_predictions.csv", index=False)

# 9) Coefficients CSV (common to report in OLS) ‚Äî kept
coef = np.ravel(model.coef_)
coef_df = pd.DataFrame({
    "feature_index": np.arange(len(coef)),
    "coefficient": coef
})
intercept_val = float(np.atleast_1d(model.intercept_)[0])
coef_df.loc[len(coef_df)] = {"feature_index": "intercept", "coefficient": intercept_val}
coef_df.to_csv(f"{out_prefix}_coefficients.csv", index=False)

# 10) Also write the per-cultivar True vs Pred mean comparison CSV (your original)
comparison_df = g_out[["Cultivar", "True_Cys", "Mean_Pred"]].rename(
    columns={"True_Cys": "True_Cys", "Mean_Pred": "Pred_Cys"}
)
comparison_df["Error"] = comparison_df["Pred_Cys"] - comparison_df["True_Cys"]
comparison_df.to_csv(f"{out_prefix}_cultivar_prediction_comparison.csv", index=False)

# 11) Separate Error Bars (Model vs Measurement) ‚Äî save with your original prefix too
def plot_pred_vs_true_with_separate_error_bars(df, targets, out_prefix):
    plt.figure(figsize=(8, 6))
    for cultivar in df["Cultivar"].unique():
        sub = df[df["Cultivar"] == cultivar]
        y_true = sub["True Cys"].values
        y_pred = sub["Predicted Cys"].values
        model_error = np.abs(y_pred - y_true)
        meas_error = cultivar_sd_map.get(str(cultivar), 0.0)

        plt.errorbar(y_true, y_pred, yerr=model_error,
                     fmt='o', label=str(cultivar), alpha=0.8, capsize=3, linewidth=1.2)
        plt.errorbar(y_true, y_pred, xerr=meas_error,
                     fmt='none', ecolor='gray', linestyle='dotted', capsize=2, linewidth=1.0)

    lo, hi = float(np.min(targets)), float(np.max(targets))
    plt.plot([lo, hi], [lo, hi], 'k--', label='Ideal')

    plt.legend(loc='upper left', frameon=True, ncol=1)
    plt.title("Predicted vs. True with Separate Error Bars (Model vs. Measurement)")
    plt.xlabel("True Cys (g/100g)"); plt.ylabel("Predicted Cys (g/100g)")
    plt.tight_layout(); plt.savefig(f"{out_prefix}_Predicted_vs_True_Separate_Errors.png", dpi=200)
    plt.close()

plot_pred_vs_true_with_separate_error_bars(df_val, Y_val, out_prefix)

# === Wrap up ===
print("\n==== OLS Linear Regression (CNN-format loader) ====")
print(f"Input dim: {X_train.shape[1]}")
print("-- Train metrics --")
for k in ["MAE", "RMSE", "R2", "MAPE_pct", "ExplainedVar", "MaxError"]:
    print(f"{k}: {train_metrics[k]:.6f}")
print("-- Val metrics --")
for k in ["MAE", "RMSE", "R2", "MAPE_pct", "ExplainedVar", "MaxError"]:
    print(f"{k}: {val_metrics[k]:.6f}")
print(f"Artifacts written with prefix: {out_prefix}")
