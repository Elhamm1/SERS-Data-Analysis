# ===== Cys-from-SERS with Simple 1D-CNN ‚Äî LOCO Cross-Validation =====

import os
import glob
import time
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import Dataset, DataLoader

from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    mean_absolute_percentage_error,
    explained_variance_score
)
from matplotlib.lines import Line2D
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# === Parameters ===
input_length   = 1496
cv_data_dir    = "../CrossValidationData"   # single directory with all *.npy
num_epochs = 100
batch_size = 32
lr = 1e-4
patience = 10
factor = 0.5
weight_decay = 1e-4  

# === Reproducibility ===
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# === Cultivar labels (values = true Cys for that cultivar) ===
cultivar_cys_map = {
    "AAC_Chrome":    0.3208,
    "AAC_Lacombe":   0.3428,
    "AAC_Liscard":   0.3181,
    "CDC_Amarillo":  0.3834,
    "CDC_Athabasca": 0.3418,
    "CDC_Canary":    0.3117,
    "CDC_Dakota":    0.3293,
    "CDC_Golden":    0.3536,
    "CDC_Greenwater":0.3446,
    "CDC_Inca":      0.3730,
    "CDC_Jasper":    0.3360,
    "CDC_Striker":   0.3575,
    "CDC_Lewochko":  0.3469,
    "CDC_Meadow":    0.3180,
    "CDC_Patrick":   0.3626,
    "CDC_Saffron":   0.3552,
    "CDC_Spectrum":  0.3948,
    "CDC_Spruce":    0.3535,
    "CDC_Tetris":    0.3531,
    "Redbat88":      0.3194,
}

# Optional SDs for later
cultivar_sd_map = {
    "AAC_Chrome":    0.0146,
    "AAC_Lacombe":   0.0401,
    "AAC_Liscard":   0.0601,
    "CDC_Amarillo":  0.0458,
    "CDC_Athabasca": 0.0215,
    "CDC_Canary":    0.0090,
    "CDC_Dakota":    0.0434,
    "CDC_Golden":    0.0425,
    "CDC_Greenwater":0.0086,
    "CDC_Inca":      0.0266,
    "CDC_Jasper":    0.0099,
    "CDC_Striker":   0.0182,
    "CDC_Lewochko":  0.0539,
    "CDC_Meadow":    0.0099,
    "CDC_Patrick":   0.0459,
    "CDC_Saffron":   0.0319,
    "CDC_Spectrum":  0.0275,
    "CDC_Spruce":    0.0028,
    "CDC_Tetris":    0.0034,
    "Redbat88":      0.0324,
}

# === Model  ===
class Cys1DCNN(nn.Module):
    def __init__(self, input_length=1496):
        super(Cys1DCNN, self).__init__()
        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm1d(16)
        self.pool1 = nn.MaxPool1d(2)

        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, padding=2)
        self.bn2 = nn.BatchNorm1d(32)
        self.pool2 = nn.MaxPool1d(2)

        self.conv3 = nn.Conv1d(32, 64, kernel_size=5, padding=2)
        self.bn3 = nn.BatchNorm1d(64)
        self.pool3 = nn.MaxPool1d(2)

        # 4th block (your updated architecture)
        self.conv4 = nn.Conv1d(64, 128, kernel_size=5, padding=2)
        self.bn4 = nn.BatchNorm1d(128)
        self.pool4 = nn.MaxPool1d(2)

        # Downsampling is now 2^4 = 16; channels = 128
        self.flattened_size = (input_length // 16) * 128
        self.fc1 = nn.Linear(self.flattened_size, 128)
        self.dropout = nn.Dropout(0.3)
        self.fc2 = nn.Linear(128, 1)

    def forward(self, x):
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))
        x = self.pool3(F.relu(self.bn3(self.conv3(x))))
        x = self.pool4(F.relu(self.bn4(self.conv4(x))))
        x = x.view(-1, self.flattened_size)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        return self.fc2(x)

# === Dataset Class ===
class SpectraDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)
        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# === Helpers ===
def find_cultivar_from_filename(filename: str, cultivar_keys):
    for key in cultivar_keys:
        if filename.startswith(key):
            return key
    return None

def load_spectra_array(file_path: str, expected_len: int) -> np.ndarray:
    """Load .npy into shape (N, L). Accept (L,), (N,L), (N,1,L)."""
    arr = np.load(file_path)
    arr = np.asarray(arr)
    if arr.ndim == 1:
        arr = arr[None, :]
    elif arr.ndim == 3 and arr.shape[1] == 1:
        arr = arr[:, 0, :]
    elif arr.ndim != 2:
        raise ValueError(f"Unexpected array shape {arr.shape} in {os.path.basename(file_path)}")
    if arr.shape[1] != expected_len:
        raise ValueError(
            f"File {os.path.basename(file_path)} has length {arr.shape[1]} != expected {expected_len}"
        )
    return arr

# === Device & training components ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
criterion = nn.SmoothL1Loss(beta=0.02)

# === Read ONLY ../CrossValidationData ===
all_files = sorted(glob.glob(os.path.join(cv_data_dir, "*.npy")))
if len(all_files) == 0:
    raise SystemExit(f"‚ö†Ô∏è No .npy files found in {cv_data_dir}.")

cultivars = list(cultivar_cys_map.keys())

# === LOCO Cross-Validation ===
all_preds, all_targets, all_cultivars = [], [], []
per_fold_results = []
per_fold_train_losses = {}
per_fold_val_losses = {}

start_time = time.time()

for fold_idx, test_cultivar in enumerate(cultivars, 1):
    print("\n" + "="*72)
    print(f"Fold {fold_idx}/{len(cultivars)} ‚Äî Test cultivar: {test_cultivar}")
    print("="*72)

    # Split files by cultivar prefix
    train_X, train_y, test_X, test_y = [], [], [], []
    for fp in all_files:
        fname = os.path.basename(fp)
        cv = find_cultivar_from_filename(fname, cultivars)
        if cv is None:
            print(f"‚ö†Ô∏è Skipping unknown file (no cultivar prefix match): {fname}")
            continue
        arr = load_spectra_array(fp, input_length)
        y_vec = np.full((arr.shape[0],), cultivar_cys_map[cv], dtype=float)
        if cv == test_cultivar:
            test_X.append(arr);  test_y.append(y_vec)
        else:
            train_X.append(arr); train_y.append(y_vec)

    if len(test_X) == 0 or len(train_X) == 0:
        print(f"Skipping fold {test_cultivar}: insufficient data.")
        continue

    X_train = np.vstack(train_X)
    Y_train = np.hstack(train_y)
    X_test  = np.vstack(test_X)
    Y_test  = np.hstack(test_y)

    # --- Sanity checks to ensure no leakage of the held-out cultivar ---

    # 1) Confirm test cultivar shows up only in TEST filenames, never in TRAIN
    train_files, test_files = [], []
    for fp in all_files:
        fname = os.path.basename(fp)
        cv = find_cultivar_from_filename(fname, cultivars)
        if cv is None:
             continue
        if cv == test_cultivar:
            test_files.append(fname)
        else:
            train_files.append(fname)

    assert all(test_cultivar not in f for f in train_files), \
        f"Leak: {test_cultivar} file found in TRAIN: {[f for f in train_files if test_cultivar in f]}"

    print(f"[CHECK] Train files: {len(train_files)} (no {test_cultivar}) | Test files: {len(test_files)} (all {test_cultivar})")

    # 2) Confirm labels (y) in TRAIN do not include the test cultivar‚Äôs target value
    y_test_value = cultivar_cys_map[test_cultivar]
    train_unique_targets = np.unique(Y_train.round(6))  # rounding guards against tiny float jitter

    assert not np.isclose(train_unique_targets, y_test_value, atol=1e-6).any(), \
        f"Leak: test cultivar's target value {y_test_value} found in TRAIN targets {train_unique_targets}"

    print(f"[CHECK] Train unique targets do NOT include {test_cultivar} ({y_test_value}) ‚úì")

    # 3) (Optional) Print a quick ‚Äúconfusion‚Äù of counts per cultivar by set
    from collections import Counter

    def expand_counts(file_list):
        return Counter([find_cultivar_from_filename(fn, cultivars) for fn in file_list])

    print("[CHECK] Train counts by cultivar:", expand_counts(train_files))
    print("[CHECK] Test counts by cultivar:", expand_counts(test_files))

    train_loader = DataLoader(SpectraDataset(X_train, Y_train), batch_size=batch_size, shuffle=True)
    test_loader  = DataLoader(SpectraDataset(X_test,  Y_test),  batch_size=batch_size, shuffle=False)

    # === Training Setup (AdamW + OneCycleLR + AMP + Huber-like SmoothL1) ===
    model = Cys1DCNN(input_length).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)

    steps_per_epoch = max(1, len(train_loader))
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=lr * 10.0,
        epochs=num_epochs,
        steps_per_epoch=steps_per_epoch,
        pct_start=0.15,
        anneal_strategy="cos",
        div_factor=10.0,
        final_div_factor=1e4
    )

    scaler = GradScaler(enabled=(device.type == "cuda"))

    train_losses, test_losses = [], []
    best_loss = float('inf')
    fold_start = time.time()

    # === Training Loop (per fold) ===
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad(set_to_none=True)
            with autocast(enabled=(device.type == "cuda")):
                output = model(X_batch)
                loss = criterion(output, y_batch)
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()
            total_loss += loss.item() * X_batch.size(0)

        epoch_train_loss = total_loss / len(train_loader.dataset)
        train_losses.append(epoch_train_loss)

        # Testing on held-out cultivar
        model.eval()
        test_loss = 0.0
        with torch.no_grad(), autocast(enabled=(device.type == "cuda")):
            for X_batch, y_batch in test_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                output = model(X_batch)
                test_loss += criterion(output, y_batch).item() * X_batch.size(0)

        epoch_test_loss = test_loss / len(test_loader.dataset)
        test_losses.append(epoch_test_loss)

        print(f"Epoch {epoch+1}/{num_epochs} | "
              f"Train Loss: {epoch_train_loss:.6f} | "
              f"Test Loss: {epoch_test_loss:.6f}")

        if epoch_test_loss < best_loss:
            best_loss = epoch_test_loss
            torch.save(model.state_dict(), f"best_{test_cultivar}.pth")

    # Save per-fold loss curve
    plt.figure()
    plt.plot(train_losses, label="Train")
    plt.plot(test_losses,  label="Test")
    plt.title("Loss Curve")
    plt.xlabel("Epoch")
    plt.ylabel("SmoothL1 Loss")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"Model_Loss_{test_cultivar}.png")
    plt.close()

    per_fold_train_losses[test_cultivar] = train_losses
    per_fold_val_losses[test_cultivar] = test_losses

    # === Evaluation on the held-out cultivar (per fold, using best model) ===
    model.load_state_dict(torch.load(f"best_{test_cultivar}.pth", map_location=device))
    model.eval()

    preds, targets = [], []
    with torch.no_grad(), autocast(enabled=(device.type == "cuda")):
        for X_batch, y_batch in test_loader:
            X_batch = X_batch.to(device)
            preds.append(model(X_batch).cpu())
            targets.append(y_batch)

    preds = torch.cat(preds).squeeze().numpy()
    targets = torch.cat(targets).squeeze().numpy()

    # === Per-fold metrics ===
    mae  = mean_absolute_error(targets, preds)
    rmse = np.sqrt(mean_squared_error(targets, preds))
    r2   = r2_score(targets, preds)

    # NEW: Print per-fold metrics separately
    print(f"\nPer-fold metrics ‚Äî {test_cultivar}:")
    print(f"MAE : {mae:.6f}")
    print(f"RMSE: {rmse:.6f}")
    print(f"R¬≤  : {r2:.6f}")

    per_fold_results.append({
        "cultivar": test_cultivar,
        "MAE": float(mae),
        "RMSE": float(rmse),
        "R2": float(r2),
        "N_test": int(len(targets)),
        "Time_sec": float(time.time() - fold_start)
    })

    # Accumulate across folds for global summaries
    all_preds.extend(preds.tolist())
    all_targets.extend(targets.tolist())
    all_cultivars.extend([test_cultivar]*len(targets))

# =========================
# Global summaries (ALL folds)
# =========================
if len(all_preds) == 0:
    raise SystemExit("No predictions collected. Check ../CrossValidationData and filename prefixes.")

all_preds   = np.array(all_preds)
all_targets = np.array(all_targets)
all_cultivars = np.array(all_cultivars)

# === Final model performance (same prints/filenames as original) ===
print("\nüìà Final Model Performance (Testing Set):")
mae  = mean_absolute_error(all_targets, all_preds)
rmse = np.sqrt(mean_squared_error(all_targets, all_preds))
r2   = r2_score(all_targets, all_preds)
print(f"MAE                 : {mae:.6f}")
print(f"RMSE                : {rmse:.6f}")
print(f"R¬≤                  : {r2:.6f}")
print(f"Max Error           : {max_error(all_targets, all_preds):{'.6f'}}")
print(f"MAPE                : {mean_absolute_percentage_error(all_targets, all_preds):.6f}")
print(f"Explained Variance  : {explained_variance_score(all_targets, all_preds):.6f}")

# === Per-spectrum table (same filename) ===
df = pd.DataFrame({"True Cys": all_targets, "Predicted Cys": all_preds})
cys_to_cultivar = {np.float32(v).item(): k for k, v in cultivar_cys_map.items()}
df["Cultivar"] = df["True Cys"].astype(np.float32).map(cys_to_cultivar)
df.to_csv("test_predictions.csv", index=False)

# === Separate Error Bars (Model vs Measurement) ‚Äî per-spectrum ===
# NEW: Use 20 distinct colors (tab20) across cultivars
unique_cvs_order = list(df["Cultivar"].unique())
palette20 = [plt.get_cmap("tab20")(i) for i in range(20)]
color_map = {cv: palette20[i % 20] for i, cv in enumerate(unique_cvs_order)}

plt.figure(figsize=(8, 6))
seen = set()
for cultivar in unique_cvs_order:
    sub = df[df["Cultivar"] == cultivar]
    y_true = sub["True Cys"].to_numpy()
    y_pred = sub["Predicted Cys"].to_numpy()

    model_error = np.abs(y_pred - y_true)
    meas_error  = float(cultivar_sd_map.get(cultivar, 0.0))

    label = cultivar if cultivar not in seen else "_nolegend_"
    seen.add(cultivar)

    # colored model error bars + points
    plt.errorbar(
        y_true, y_pred, yerr=model_error,
        fmt='o', label=label, alpha=0.8, capsize=3, elinewidth=1.2,
        color=color_map[cultivar]
    )
    # dotted helper (kept gray)
    plt.errorbar(
        y_true, y_pred,
        fmt='none', ecolor='gray', linestyle='dotted',
        capsize=2, elinewidth=1.0
    )

mn = float(min(df["True Cys"].min(), df["Predicted Cys"].min()))
mx = float(max(df["True Cys"].max(), df["Predicted Cys"].max()))
plt.plot([mn, mx], [mn, mx], 'k--', label="_nolegend_")
plt.title("Predicted vs. True with Separate Error Bars (Model vs. Measurement)")
plt.xlabel("True Cys (g/100g)")
plt.ylabel("Predicted Cys (g/100g)")
plt.grid(True, alpha=0.2)
plt.legend(loc="upper left", bbox_to_anchor=(1.02, 1.0), borderaxespad=0.0, frameon=True)
plt.tight_layout()
plt.savefig("Predicted_vs_True_Separate_Errors.png")
plt.close()

# === Separate Error Bars (MEAN per cultivar) ===
g = (df.groupby("Cultivar")
       .agg(True_Cys=("True Cys", "first"),
            Mean_Pred=("Predicted Cys", "mean"),
            Std_Pred=("Predicted Cys", "std"),
            N=("Predicted Cys", "size"))
       .reset_index())

plt.figure(figsize=(10, 6))
use_standard_error = False  # True => show SE; False => SD

for _, row in g.iterrows():
    cultivar = row["Cultivar"]
    x = float(row["True_Cys"])
    y = float(row["Mean_Pred"])
    yerr = float(row["Std_Pred"] / np.sqrt(row["N"]) if use_standard_error else row["Std_Pred"])
    xerr = float(cultivar_sd_map.get(cultivar, 0.0))

    # NEW: two alternating colors across cultivars (reuse map; fallback to first color)
    c = color_map.get(cultivar, palette20[0])

    plt.errorbar(
        x, y, yerr=yerr, fmt='o', capsize=3, elinewidth=1.2,
        label=cultivar, alpha=0.9, color=c
    )
    plt.errorbar(
        x, y, fmt='none', ecolor='gray',
        linestyle='dotted', capsize=2, elinewidth=1.0
    )

mn = float(min(g["True_Cys"].min(), g["Mean_Pred"].min()))
mx = float(max(g["True_Cys"].max(), g["Mean_Pred"].max()))
ax = plt.gca()
ax.set_aspect('equal', adjustable='box')
pad = 0.002
ax.set_xlim(mn - pad, mx + pad)
ax.set_ylim(mn - pad, mx + pad)
ax.plot([mn - pad, mx + pad], [mn - pad, mx + pad], 'k--', label='Ideal (y=x)')

X = g["True_Cys"].to_numpy().reshape(-1, 1)
y_means = g["Mean_Pred"].to_numpy()
reg = LinearRegression().fit(X, y_means)
slope = float(reg.coef_[0])
intercept = float(reg.intercept_)
x_line = np.linspace(mn - pad, mx + pad, 200)
# keep a distinct color for fit line (uses second of the two colors)
ax.plot(x_line, slope * x_line + intercept, '-', linewidth=2, color='k',
        label=f'Fit: y={slope:.3f}x+{intercept:.3f}')

r2_means = r2_score(g["True_Cys"], g["Mean_Pred"])
r2_fit   = r2_score(y_means, reg.predict(X))

y_std   = all_targets.std(ddof=1)
y_range = all_targets.max() - all_targets.min()
rmse_all = np.sqrt(mean_squared_error(all_targets, all_preds))

plt.title("Mean Predicted vs True HPLC per Cultivar")
plt.xlabel("True HPLC Cys (g/100g)")
plt.ylabel("Mean Predicted Cys (g/100g)")
ax.legend(loc="upper left", bbox_to_anchor=(1.02, 1.0), borderaxespad=0.0, frameon=True)
plt.tight_layout(rect=(0.0, 0.0, 0.80, 1.0))
plt.savefig("Predicted_vs_True_Separate_Errors_MEAN.png")
plt.close()

# --- Mean-level metrics + bar chart ---
mae_means  = mean_absolute_error(g["True_Cys"], g["Mean_Pred"])
rmse_means = np.sqrt(mean_squared_error(g["True_Cys"], g["Mean_Pred"]))
print("\nPer-cultivar mean metrics:")
print(f"R¬≤(means) : {r2_means:.6f}")
print(f"MAE(means): {mae_means:.6f}")
print(f"RMSE(means): {rmse_means:.6f}")

# NEW: Report R¬≤(means) ¬± standard deviation of all per-fold R¬≤ (uncertainty on R¬≤)
if len(per_fold_results) > 0:
    r2_values = np.array([res["R2"] for res in per_fold_results], dtype=float)
    r2_sd = float(np.std(r2_values, ddof=1)) if len(r2_values) > 1 else 0.0
    print(f"R¬≤(means) ¬± SD(per-fold R¬≤): {r2_means:.6f} ¬± {r2_sd:.6f}")

print("\nNormalized error (per-spectrum):")
print(f"RMSE / std(y):   {rmse_all / y_std:.3f}")
print(f"RMSE / range(y): {rmse_all / y_range:.3f}")

SHOW_HPLC_SD = False
yerr_pred = (g["Std_Pred"] / np.sqrt(g["N"])) if use_standard_error else g["Std_Pred"]
yerr_pred = np.nan_to_num(yerr_pred.to_numpy(), nan=0.0)
yerr_true = np.array([cultivar_sd_map.get(cv, 0.0) for cv in g["Cultivar"]]) if SHOW_HPLC_SD else None

plt.figure(figsize=(10, 6))
x_pos = np.arange(len(g))
bar_w = 0.38
plt.bar(x_pos - bar_w/2, g["True_Cys"], width=bar_w, label="True HPLC",
        yerr=yerr_true, capsize=3 if SHOW_HPLC_SD else 0)
plt.bar(x_pos + bar_w/2, g["Mean_Pred"], width=bar_w,
        label=f"Mean Predicted",
        yerr=yerr_pred, capsize=3)
plt.xticks(x_pos, g["Cultivar"], rotation=45, ha="right")
plt.ylabel("Cys Concentration (g/100g)")
plt.title("Mean Predicted vs True HPLC values per Cultivar")
plt.grid(axis='y', alpha=0.2)
plt.legend()
plt.tight_layout()
plt.savefig("bar_mean_true_vs_pred.png")
plt.close()

# Save per-cultivar table with absolute errors
g_out = g.copy()
g_out["Abs_Error"]    = (g_out["Mean_Pred"] - g_out["True_Cys"]).abs()
g_out["Pct_Error_%"]  = 100.0 * g_out["Abs_Error"] / g_out["True_Cys"]
g_out["SE_Pred"]      = g_out["Std_Pred"] / np.sqrt(g_out["N"])
cols = ["Cultivar", "True_Cys", "Mean_Pred", "Abs_Error", "Pct_Error_%", "N", "Std_Pred", "SE_Pred"]
g_out[cols].to_csv("mean_vs_true_by_cultivar.csv", index=False)

print("\nSaved bar plot -> bar_mean_true_vs_pred.png")
print("Saved per-cultivar summary -> mean_vs_true_by_cultivar.csv")

# === Predicted vs True scatter for the (global) test set ===
plt.figure(figsize=(6, 6))
plt.scatter(all_targets, all_preds, alpha=0.7)
mn, mx = min(all_targets.min(), all_preds.min()), max(all_targets.max(), all_preds.max())
plt.plot([mn, mx], [mn, mx], 'k--', label='Ideal')
plt.xlabel("True Cys (g/100g)")
plt.ylabel("Predicted Cys (g/100g)")
plt.title("Predicted vs True Cys (Testing Set)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("calibration_plot_testing_set.png")
plt.close()

# === Aggregate "Model_Loss.png" across folds (average per-epoch) ===
if len(per_fold_train_losses) > 0:
    tmat = np.array([per_fold_train_losses[k] for k in per_fold_train_losses])
    vmat = np.array([per_fold_val_losses[k]   for k in per_fold_val_losses])
    mean_train = tmat.mean(axis=0)
    mean_val   = vmat.mean(axis=0)

    plt.figure()
    plt.plot(mean_train, label="Train")
    plt.plot(mean_val,   label="Test")
    plt.title("Loss Curve")
    plt.xlabel("Epoch")
    plt.ylabel("SmoothL1 Loss")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig("Model_Loss.png")
    plt.close()

# Save per-fold metrics (extra)
pd.DataFrame(per_fold_results).to_csv("per_fold_metrics.csv", index=False)

print(f"\n‚è±Ô∏è Total run time: {time.time() - start_time:.2f} seconds")
