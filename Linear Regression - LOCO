# ===== Cys-from-SERS ‚Äî LOCO Cross-Validation with Linear Regression (OLS) =====

import os
import glob
import time
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    mean_absolute_percentage_error,
    explained_variance_score
)

# === Parameters ===
input_length   = 1496
cv_data_dir    = "../../CrossValidationData"   # single directory with all *.npy

# Keep epoch-style knobs so the loss plots/arrays match your previous script
num_epochs   = 100   # used only to draw flat loss curves for OLS
batch_size   = 32    # unused (kept for parity)
lr           = 1e-4  # unused (kept for parity)
patience     = 10    # unused (kept for parity)
factor       = 0.5   # unused (kept for parity)
weight_decay = 1e-4  # unused (kept for parity)

# === Reproducibility ===
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# === Cultivar labels (values = true Cys for that cultivar) ===
cultivar_cys_map = {
    "AAC_Chrome":    0.3208,
    "AAC_Lacombe":   0.3428,
    "AAC_Liscard":   0.3181,
    "CDC_Amarillo":  0.3834,
    "CDC_Athabasca": 0.3418,
    "CDC_Canary":    0.3117,
    "CDC_Dakota":    0.3293,
    "CDC_Golden":    0.3536,
    "CDC_Greenwater":0.3446,
    "CDC_Inca":      0.3730,
    "CDC_Jasper":    0.3360,
    "CDC_Striker":   0.3575,
    "CDC_Lewochko":  0.3469,
    "CDC_Meadow":    0.3180,
    "CDC_Patrick":   0.3626,
    "CDC_Saffron":   0.3552,
    "CDC_Spectrum":  0.3948,
    "CDC_Spruce":    0.3535,
    "CDC_Tetris":    0.3531,
    "Redbat88":      0.3194,
}

# Optional SDs for later (HPLC measurement SDs)
cultivar_sd_map = {
    "AAC_Chrome":    0.0146,
    "AAC_Lacombe":   0.0401,
    "AAC_Liscard":   0.0601,
    "CDC_Amarillo":  0.0458,
    "CDC_Athabasca": 0.0215,
    "CDC_Canary":    0.0090,
    "CDC_Dakota":    0.0434,
    "CDC_Golden":    0.0425,
    "CDC_Greenwater":0.0086,
    "CDC_Inca":      0.0266,
    "CDC_Jasper":    0.0099,
    "CDC_Striker":   0.0182,
    "CDC_Lewochko":  0.0539,
    "CDC_Meadow":    0.0099,
    "CDC_Patrick":   0.0459,
    "CDC_Saffron":   0.0319,
    "CDC_Spectrum":  0.0275,
    "CDC_Spruce":    0.0028,
    "CDC_Tetris":    0.0034,
    "Redbat88":      0.0324,
}

# === Helpers ===
def find_cultivar_from_filename(filename: str, cultivar_keys):
    for key in cultivar_keys:
        if filename.startswith(key):
            return key
    return None

def load_spectra_array(file_path: str, expected_len: int) -> np.ndarray:
    """Load .npy into shape (N, L). Accept (L,), (N,L), (N,1,L)."""
    arr = np.load(file_path)
    arr = np.asarray(arr)
    if arr.ndim == 1:
        arr = arr[None, :]
    elif arr.ndim == 3 and arr.shape[1] == 1:
        arr = arr[:, 0, :]
    elif arr.ndim != 2:
        raise ValueError(f"Unexpected array shape {arr.shape} in {os.path.basename(file_path)}")
    if arr.shape[1] != expected_len:
        raise ValueError(
            f"File {os.path.basename(file_path)} has length {arr.shape[1]} != expected {expected_len}"
        )
    return arr

# === Read ONLY ../CrossValidationData ===
all_files = sorted(glob.glob(os.path.join(cv_data_dir, "*.npy")))
if len(all_files) == 0:
    raise SystemExit(f"‚ö†Ô∏è No .npy files found in {cv_data_dir}.")

cultivars = list(cultivar_cys_map.keys())

# === LOCO Cross-Validation ===
all_preds, all_targets, all_cultivars = [], [], []
per_fold_results = []
per_fold_train_losses = {}
per_fold_val_losses   = {}

start_time = time.time()

for fold_idx, test_cultivar in enumerate(cultivars, 1):
    print("\n" + "="*72)
    print(f"Fold {fold_idx}/{len(cultivars)} ‚Äî Test cultivar: {test_cultivar}")
    print("="*72)

    # Split files by cultivar prefix
    train_X, train_y, test_X, test_y = [], [], [], []
    for fp in all_files:
        fname = os.path.basename(fp)
        cv = find_cultivar_from_filename(fname, cultivars)
        if cv is None:
            print(f"‚ö†Ô∏è Skipping unknown file (no cultivar prefix match): {fname}")
            continue
        arr = load_spectra_array(fp, input_length)
        y_vec = np.full((arr.shape[0],), cultivar_cys_map[cv], dtype=float)
        if cv == test_cultivar:
            test_X.append(arr);  test_y.append(y_vec)
        else:
            train_X.append(arr); train_y.append(y_vec)

    if len(test_X) == 0 or len(train_X) == 0:
        print(f"Skipping fold {test_cultivar}: insufficient data.")
        continue

    X_train = np.vstack(train_X)
    Y_train = np.hstack(train_y)
    X_test  = np.vstack(test_X)
    Y_test  = np.hstack(test_y)

    # --- Sanity checks to ensure no leakage of the held-out cultivar ---
    # 1) Confirm test cultivar shows up only in TEST filenames, never in TRAIN
    train_files, test_files = [], []
    for fp in all_files:
        fname = os.path.basename(fp)
        cv = find_cultivar_from_filename(fname, cultivars)
        if cv is None:
             continue
        if cv == test_cultivar:
            test_files.append(fname)
        else:
            train_files.append(fname)

    assert all(test_cultivar not in f for f in train_files), \
        f"Leak: {test_cultivar} file found in TRAIN: {[f for f in train_files if test_cultivar in f]}"

    print(f"[CHECK] Train files: {len(train_files)} (no {test_cultivar}) | Test files: {len(test_files)} (all {test_cultivar})")

    # 2) Confirm labels (y) in TRAIN do not include the test cultivar‚Äôs target value
    y_test_value = cultivar_cys_map[test_cultivar]
    train_unique_targets = np.unique(Y_train.round(6))
    assert not np.isclose(train_unique_targets, y_test_value, atol=1e-6).any(), \
        f"Leak: test cultivar's target value {y_test_value} found in TRAIN targets {train_unique_targets}"

    print(f"[CHECK] Train unique targets do NOT include {test_cultivar} ({y_test_value}) ‚úì")

    from collections import Counter
    def expand_counts(file_list):
        return Counter([find_cultivar_from_filename(fn, cultivars) for fn in file_list])
    print("[CHECK] Train counts by cultivar:", expand_counts(train_files))
    print("[CHECK] Test counts by cultivar:", expand_counts(test_files))

    # === Linear Regression (OLS) in place of CNN ===
    model = LinearRegression()  # fit_intercept=True by default
    fit_start = time.time()
    model.fit(X_train, Y_train)
    fit_time = time.time() - fit_start
    print(f"‚è±Ô∏è Total fit run time: fit_time seconds")

    # Predictions
    ytr_pred = model.predict(X_train)
    yte_pred = model.predict(X_test)

    # Per-epoch (simulated) loss arrays for plotting ‚Äî flat lines
    train_mse = float(mean_squared_error(Y_train, ytr_pred))
    test_mse  = float(mean_squared_error(Y_test,  yte_pred))
    train_losses = [train_mse] * num_epochs
    test_losses  = [test_mse]  * num_epochs

    # Save per-fold loss curve
    plt.figure()
    plt.plot(train_losses, label="Train")
    plt.plot(test_losses,  label="Test")
    plt.title("Loss Curve (OLS, constant across epochs)")
    plt.xlabel("Epoch")
    plt.ylabel("MSE")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"Model_Loss_{test_cultivar}.png")
    plt.close()

    per_fold_train_losses[test_cultivar] = train_losses
    per_fold_val_losses[test_cultivar]   = test_losses

    # === Evaluation on the held-out cultivar (per fold) ===
    mae  = mean_absolute_error(Y_test, yte_pred)
    rmse = np.sqrt(mean_squared_error(Y_test, yte_pred))
    r2   = r2_score(Y_test, yte_pred)

    print(f"\nPer-fold metrics ‚Äî {test_cultivar}:")
    print(f"MAE : {mae:.6f}")
    print(f"RMSE: {rmse:.6f}")
    print(f"R¬≤  : {r2:.6f}")

    per_fold_results.append({
        "cultivar": test_cultivar,
        "MAE": float(mae),
        "RMSE": float(rmse),
        "R2": float(r2),
        "N_test": int(len(Y_test)),
        "Time_sec": float(fit_time)
    })

    # Optional: write per-fold OLS coefficients for inspection
    coef = np.ravel(model.coef_)
    coef_df = pd.DataFrame({
        "feature_index": np.arange(len(coef), dtype=object),
        "coefficient": coef
    })
    intercept_val = float(np.atleast_1d(model.intercept_)[0])
    coef_df.loc[len(coef_df)] = {"feature_index": "intercept", "coefficient": intercept_val}
    coef_df.to_csv(f"ols_coefficients_{test_cultivar}.csv", index=False)

    # Accumulate across folds for global summaries
    all_preds.extend(yte_pred.tolist())
    all_targets.extend(Y_test.tolist())
    all_cultivars.extend([test_cultivar]*len(Y_test))

# =========================
# Global summaries (ALL folds)
# =========================
if len(all_preds) == 0:
    raise SystemExit("No predictions collected. Check ../CrossValidationData and filename prefixes.")

all_preds     = np.array(all_preds)
all_targets   = np.array(all_targets)
all_cultivars = np.array(all_cultivars)

# === Final model performance (same prints/filenames as original) ===
print("\nüìà Final Model Performance (Testing Set):")
mae  = mean_absolute_error(all_targets, all_preds)
rmse = np.sqrt(mean_squared_error(all_targets, all_preds))
r2   = r2_score(all_targets, all_preds)
print(f"MAE                 : {mae:.6f}")
print(f"RMSE                : {rmse:.6f}")
print(f"R¬≤                  : {r2:.6f}")
print(f"Max Error           : {max_error(all_targets, all_preds):{'.6f'}}")
print(f"MAPE                : {mean_absolute_percentage_error(all_targets, all_preds):.6f}")
print(f"Explained Variance  : {explained_variance_score(all_targets, all_preds):.6f}")

# === Per-spectrum table (same filename) ===
df = pd.DataFrame({"True Cys": all_targets, "Predicted Cys": all_preds})
cys_to_cultivar = {np.float32(v).item(): k for k, v in cultivar_cys_map.items()}
df["Cultivar"] = df["True Cys"].astype(np.float32).map(cys_to_cultivar)
df.to_csv("test_predictions.csv", index=False)

# === Separate Error Bars (Model vs Measurement) ‚Äî per-spectrum ===
unique_cvs_order = list(df["Cultivar"].unique())
palette20 = [plt.get_cmap("tab20")(i) for i in range(20)]
color_map = {cv: palette20[i % 20] for i, cv in enumerate(unique_cvs_order)}

plt.figure(figsize=(8, 6))
seen = set()
for cultivar in unique_cvs_order:
    sub = df[df["Cultivar"] == cultivar]
    y_true = sub["True Cys"].to_numpy()
    y_pred = sub["Predicted Cys"].to_numpy()

    model_error = np.abs(y_pred - y_true)
    meas_error  = float(cultivar_sd_map.get(cultivar, 0.0))

    label = cultivar if cultivar not in seen else "_nolegend_"
    seen.add(cultivar)

    plt.errorbar(
        y_true, y_pred, yerr=model_error,
        fmt='o', label=label, alpha=0.8, capsize=3, elinewidth=1.2,
        color=color_map[cultivar]
    )
    plt.errorbar(
        y_true, y_pred,
        fmt='none', ecolor='gray', linestyle='dotted',
        capsize=2, elinewidth=1.0
    )

mn = float(min(df["True Cys"].min(), df["Predicted Cys"].min()))
mx = float(max(df["True Cys"].max(), df["Predicted Cys"].max()))
plt.plot([mn, mx], [mn, mx], 'k--', label="_nolegend_")
plt.title("Predicted vs. True with Separate Error Bars (Model vs. Measurement)")
plt.xlabel("True Cys (g/100g)")
plt.ylabel("Predicted Cys (g/100g)")
plt.grid(True, alpha=0.2)
plt.legend(loc="upper left", bbox_to_anchor=(1.02, 1.0), borderaxespad=0.0, frameon=True)
plt.tight_layout()
plt.savefig("Predicted_vs_True_Separate_Errors.png")
plt.close()

# === Separate Error Bars (MEAN per cultivar) ===
g = (df.groupby("Cultivar")
       .agg(True_Cys=("True Cys", "first"),
            Mean_Pred=("Predicted Cys", "mean"),
            Std_Pred=("Predicted Cys", "std"),
            N=("Predicted Cys", "size"))
       .reset_index())

plt.figure(figsize=(10, 6))
use_standard_error = False  # True => show SE; False => SD

for _, row in g.iterrows():
    cultivar = row["Cultivar"]
    x = float(row["True_Cys"])
    y = float(row["Mean_Pred"])
    yerr = float(row["Std_Pred"] / np.sqrt(row["N"]) if use_standard_error else row["Std_Pred"])
    xerr = float(cultivar_sd_map.get(cultivar, 0.0))

    c = color_map.get(cultivar, palette20[0])

    plt.errorbar(
        x, y, yerr=yerr, fmt='o', capsize=3, elinewidth=1.2,
        label=cultivar, alpha=0.9, color=c
    )
    plt.errorbar(
        x, y, fmt='none', ecolor='gray',
        linestyle='dotted', capsize=2, elinewidth=1.0
    )

mn = float(min(g["True_Cys"].min(), g["Mean_Pred"].min()))
mx = float(max(g["True_Cys"].max(), g["Mean_Pred"].max()))
ax = plt.gca()
ax.set_aspect('equal', adjustable='box')
pad = 0.002
ax.set_xlim(mn - pad, mx + pad)
ax.set_ylim(mn - pad, mx + pad)
ax.plot([mn - pad, mx + pad], [mn - pad, mx + pad], 'k--', label='Ideal (y=x)')

# OLS on cultivar means (diagnostic)
X = g["True_Cys"].to_numpy().reshape(-1, 1)
y_means = g["Mean_Pred"].to_numpy()
reg = LinearRegression().fit(X, y_means)
slope = float(reg.coef_[0])
intercept = float(reg.intercept_)
x_line = np.linspace(mn - pad, mx + pad, 200)
ax.plot(x_line, slope * x_line + intercept, '-', linewidth=2, color='k',
        label=f'Fit: y={slope:.3f}x+{intercept:.3f}')

r2_means = r2_score(g["True_Cys"], g["Mean_Pred"])
r2_fit   = r2_score(y_means, reg.predict(X))

plt.title("Mean Predicted vs True HPLC per Cultivar")
plt.xlabel("True HPLC Cys (g/100g)")
plt.ylabel("Mean Predicted Cys (g/100g)")
ax.legend(loc="upper left", bbox_to_anchor=(1.02, 1.0), borderaxespad=0.0, frameon=True)
plt.tight_layout(rect=(0.0, 0.0, 0.80, 1.0))
plt.savefig("Predicted_vs_True_Separate_Errors_MEAN.png")
plt.close()

# --- Mean-level metrics + bar chart ---
mae_means  = mean_absolute_error(g["True_Cys"], g["Mean_Pred"])
rmse_means = np.sqrt(mean_squared_error(g["True_Cys"], g["Mean_Pred"]))
print("\nPer-cultivar mean metrics:")
print(f"R¬≤(means) : {r2_means:.6f}")
print(f"MAE(means): {mae_means:.6f}")
print(f"RMSE(means): {rmse_means:.6f}")

# Report R¬≤(means) ¬± SD(per-fold R¬≤) like before
if len(per_fold_results) > 0:
    r2_values = np.array([res["R2"] for res in per_fold_results], dtype=float)
    r2_sd = float(np.std(r2_values, ddof=1)) if len(r2_values) > 1 else 0.0
    print(f"R¬≤(means) ¬± SD(per-fold R¬≤): {r2_means:.6f} ¬± {r2_sd:.6f}")

print("\nNormalized error (per-spectrum):")
y_std   = all_targets.std(ddof=1)
y_range = all_targets.max() - all_targets.min()
rmse_all = np.sqrt(mean_squared_error(all_targets, all_preds))
print(f"RMSE / std(y):   {rmse_all / y_std:.3f}")
print(f"RMSE / range(y): {rmse_all / y_range:.3f}")

SHOW_HPLC_SD = False
yerr_pred = (g["Std_Pred"] / np.sqrt(g["N"])) if use_standard_error else g["Std_Pred"]
yerr_pred = np.nan_to_num(yerr_pred.to_numpy(), nan=0.0)
yerr_true = np.array([cultivar_sd_map.get(cv, 0.0) for cv in g["Cultivar"]]) if SHOW_HPLC_SD else None

plt.figure(figsize=(10, 6))
x_pos = np.arange(len(g))
bar_w = 0.38
plt.bar(x_pos - bar_w/2, g["True_Cys"], width=bar_w, label="True HPLC",
        yerr=yerr_true, capsize=3 if SHOW_HPLC_SD else 0)
plt.bar(x_pos + bar_w/2, g["Mean_Pred"], width=bar_w,
        label=f"Mean Predicted",
        yerr=yerr_pred, capsize=3)
plt.xticks(x_pos, g["Cultivar"], rotation=45, ha="right")
plt.ylabel("Cys Concentration (g/100g)")
plt.title("Mean Predicted vs True HPLC values per Cultivar")
plt.grid(axis='y', alpha=0.2)
plt.legend()
plt.tight_layout()
plt.savefig("bar_mean_true_vs_pred.png")
plt.close()

# Save per-cultivar table with absolute errors
g_out = g.copy()
g_out["Abs_Error"]    = (g_out["Mean_Pred"] - g_out["True_Cys"]).abs()
g_out["Pct_Error_%"]  = 100.0 * g_out["Abs_Error"] / g_out["True_Cys"]
g_out["SE_Pred"]      = g_out["Std_Pred"] / np.sqrt(g_out["N"])
cols = ["Cultivar", "True_Cys", "Mean_Pred", "Abs_Error", "Pct_Error_%", "N", "Std_Pred", "SE_Pred"]
g_out[cols].to_csv("mean_vs_true_by_cultivar.csv", index=False)

print("\nSaved bar plot -> bar_mean_true_vs_pred.png")
print("Saved per-cultivar summary -> mean_vs_true_by_cultivar.csv")

# === Predicted vs True scatter for the (global) test set ===
plt.figure(figsize=(6, 6))
plt.scatter(all_targets, all_preds, alpha=0.7)
mn, mx = min(all_targets.min(), all_preds.min()), max(all_targets.max(), all_preds.max())
plt.plot([mn, mx], [mn, mx], 'k--', label='Ideal')
plt.xlabel("True Cys (g/100g)")
plt.ylabel("Predicted Cys (g/100g)")
plt.title("Predicted vs True Cys (Testing Set)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("calibration_plot_testing_set.png")
plt.close()

# === Aggregate "Model_Loss.png" across folds (average per-epoch) ===
if len(per_fold_train_losses) > 0:
    tmat = np.array([per_fold_train_losses[k] for k in per_fold_train_losses])
    vmat = np.array([per_fold_val_losses[k]   for k in per_fold_val_losses])
    mean_train = tmat.mean(axis=0)
    mean_val   = vmat.mean(axis=0)

    plt.figure()
    plt.plot(mean_train, label="Train")
    plt.plot(mean_val,   label="Test")
    plt.title("Loss Curve (OLS, constant across epochs)")
    plt.xlabel("Epoch")
    plt.ylabel("MSE")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig("Model_Loss.png")
    plt.close()

# Save per-fold metrics (extra)
pd.DataFrame(per_fold_results).to_csv("per_fold_metrics.csv", index=False)

print(f"\n‚è±Ô∏è Total run time: {time.time() - start_time:.2f} seconds")
