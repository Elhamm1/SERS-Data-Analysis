# ===== Cys-from-SERS with PLS Regression ‚Äî LOCO Cross-Validation  =====

import os
import glob
import time
import random
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from collections import Counter

# Scikit-learn
from sklearn.cross_decomposition import PLSRegression
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score, max_error,
    mean_absolute_percentage_error, explained_variance_score
)
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import TransformedTargetRegressor

# === Parameters ===
input_length   = 1496
cv_data_dir    = "../../CrossValidationData"   
SEED = 42
MAX_COMPONENTS_CAP = 25        # upper cap for n_components search
INNER_CV_SPLITS   = 5          # inner CV for component selection
USE_STANDARD_ERROR = False     # for MEAN plot errorbars

# === Reproducibility ===
random.seed(SEED)
np.random.seed(SEED)

# === Cultivar labels (values = true Cys for that cultivar) ===
cultivar_cys_map = {
    "AAC_Chrome":    0.3208,
    "AAC_Lacombe":   0.3428,
    "AAC_Liscard":   0.3181,
    "CDC_Amarillo":  0.3834,
    "CDC_Athabasca": 0.3418,
    "CDC_Canary":    0.3117,
    "CDC_Dakota":    0.3293,
    "CDC_Golden":    0.3536,
    "CDC_Greenwater":0.3446,
    "CDC_Inca":      0.3730,
    "CDC_Jasper":    0.3360,
    "CDC_Striker":   0.3575,
    "CDC_Lewochko":  0.3469,
    "CDC_Meadow":    0.3180,
    "CDC_Patrick":   0.3626,
    "CDC_Saffron":   0.3552,
    "CDC_Spectrum":  0.3948,
    "CDC_Spruce":    0.3535,
    "CDC_Tetris":    0.3531,
    "Redbat88":      0.3194,
}

# Optional SDs for later (HPLC measurement SDs)
cultivar_sd_map = {
    "AAC_Chrome":    0.0146, "AAC_Lacombe": 0.0401, "AAC_Liscard": 0.0601,
    "CDC_Amarillo":  0.0458, "CDC_Athabasca": 0.0215, "CDC_Canary": 0.0090,
    "CDC_Dakota":    0.0434, "CDC_Golden": 0.0425, "CDC_Greenwater": 0.0086,
    "CDC_Inca":      0.0266, "CDC_Jasper": 0.0099, "CDC_Striker": 0.0182,
    "CDC_Lewochko":  0.0539, "CDC_Meadow": 0.0099, "CDC_Patrick": 0.0459,
    "CDC_Saffron":   0.0319, "CDC_Spectrum": 0.0275, "CDC_Spruce": 0.0028,
    "CDC_Tetris":    0.0034, "Redbat88": 0.0324,
}

# === Helpers ===
def find_cultivar_from_filename(filename: str, cultivar_keys):
    for key in cultivar_keys:
        if filename.startswith(key):
            return key
    return None

def load_spectra_array(file_path: str, expected_len: int) -> np.ndarray:
    """Load .npy into shape (N, L). Accept (L,), (N,L), (N,1,L)."""
    arr = np.load(file_path)
    arr = np.asarray(arr)
    if arr.ndim == 1:
        arr = arr[None, :]
    elif arr.ndim == 3 and arr.shape[1] == 1:
        arr = arr[:, 0, :]
    elif arr.ndim != 2:
        raise ValueError(f"Unexpected array shape {arr.shape} in {os.path.basename(file_path)}")
    if arr.shape[1] != expected_len:
        raise ValueError(
            f"File {os.path.basename(file_path)} has length {arr.shape[1]} != expected {expected_len}"
        )
    return arr

def safe_r2(y_true, y_pred, atol=1e-12):
    """R¬≤ is undefined if y_true is (numerically) constant. Return np.nan in that case."""
    y_true = np.asarray(y_true)
    if np.allclose(y_true, y_true[0], atol=atol):
        return np.nan
    return r2_score(y_true, y_pred)

# === Read ONLY CrossValidationData ===
all_files = sorted(glob.glob(os.path.join(cv_data_dir, "*.npy")))
if len(all_files) == 0:
    raise SystemExit(f"‚ö†Ô∏è No .npy files found in {cv_data_dir}.")

cultivars = list(cultivar_cys_map.keys())

# === LOCO Cross-Validation ===
all_preds, all_targets, all_cultivars = [], [], []
per_fold_results = []
per_fold_train_losses = {}  # per-fold list of train MSE vs n_components
per_fold_val_losses   = {}  # per-fold list of test  MSE vs n_components

start_time = time.time()

for fold_idx, test_cultivar in enumerate(cultivars, 1):
    print("\n" + "="*72)
    print(f"Fold {fold_idx}/{len(cultivars)} ‚Äî Test cultivar: {test_cultivar}")
    print("="*72)

    # Split files by cultivar prefix
    train_X, train_y, test_X, test_y = [], [], [], []
    for fp in all_files:
        fname = os.path.basename(fp)
        cv = find_cultivar_from_filename(fname, cultivars)
        if cv is None:
            print(f"‚ö†Ô∏è Skipping unknown file (no cultivar prefix match): {fname}")
            continue
        arr = load_spectra_array(fp, input_length)
        y_vec = np.full((arr.shape[0],), cultivar_cys_map[cv], dtype=float)
        if cv == test_cultivar:
            test_X.append(arr);  test_y.append(y_vec)
        else:
            train_X.append(arr); train_y.append(y_vec)

    if len(test_X) == 0 or len(train_X) == 0:
        print(f"Skipping fold {test_cultivar}: insufficient data.")
        continue

    X_train = np.vstack(train_X).astype(np.float64, copy=False)
    Y_train = np.hstack(train_y).astype(np.float64, copy=False)
    X_test  = np.vstack(test_X).astype(np.float64, copy=False)
    Y_test  = np.hstack(test_y).astype(np.float64, copy=False)

    # --- Sanity checks to ensure no leakage of the held-out cultivar ---
    train_files, test_files = [], []
    for fp in all_files:
        fname = os.path.basename(fp)
        cv = find_cultivar_from_filename(fname, cultivars)
        if cv is None:
            continue
        if cv == test_cultivar:
            test_files.append(fname)
        else:
            train_files.append(fname)

    assert all(test_cultivar not in f for f in train_files), \
        f"Leak: {test_cultivar} file found in TRAIN: {[f for f in train_files if test_cultivar in f]}"
    print(f"[CHECK] Train files: {len(train_files)} (no {test_cultivar}) | Test files: {len(test_files)} (all {test_cultivar})")

    y_test_value = cultivar_cys_map[test_cultivar]
    train_unique_targets = np.unique(Y_train.round(6))  # rounding guards against tiny float jitter
    assert not np.isclose(train_unique_targets, y_test_value, atol=1e-6).any(), \
        f"Leak: test cultivar's target value {y_test_value} found in TRAIN targets {train_unique_targets}"
    print(f"[CHECK] Train unique targets do NOT include {test_cultivar} ({y_test_value}) ‚úì")

    def expand_counts(file_list):
        return Counter([find_cultivar_from_filename(fn, cultivars) for fn in file_list])
    print("[CHECK] Train counts by cultivar:", expand_counts(train_files))
    print("[CHECK] Test counts by cultivar:", expand_counts(test_files))

    # === Fold-local PLS pipeline: Standardize X; Center y ===
    pls = PLSRegression(scale=False)
    pipe = Pipeline([
        ("x_scaler", StandardScaler(with_mean=True, with_std=True)),
        ("pls", pls),
    ])
    # Center y (mean-only) on TRAIN; mean added back automatically at predict time
    ttr = TransformedTargetRegressor(
        regressor=pipe,
        transformer=StandardScaler(with_mean=True, with_std=False)
    )

    # === Inner CV over n_components on TRAIN only ===
    max_components = max(1, min(MAX_COMPONENTS_CAP, X_train.shape[1], X_train.shape[0] - 1))
    inner_cv = KFold(n_splits=INNER_CV_SPLITS, shuffle=True, random_state=SEED)
    param_grid = {"regressor__pls__n_components": list(range(1, max_components + 1))}
    search = GridSearchCV(
        estimator=ttr,
        param_grid=param_grid,
        scoring="neg_mean_squared_error",
        cv=inner_cv,
        n_jobs=-1
    )

    fold_start = time.time()
    search.fit(X_train, Y_train)
    best_model = search.best_estimator_
    best_k = int(search.best_params_["regressor__pls__n_components"])

    # === "Loss curves" across component counts (acts like epochs) ===
    # Refit fold-local pipeline for each k to compute Train/Test MSE curves
    train_losses, test_losses = [], []
    for k in range(1, max_components + 1):
        model_k = TransformedTargetRegressor(
            regressor=Pipeline([
                ("x_scaler", StandardScaler(with_mean=True, with_std=True)),
                ("pls", PLSRegression(n_components=k, scale=False)),
            ]),
            transformer=StandardScaler(with_mean=True, with_std=False)
        )
        model_k.fit(X_train, Y_train)
        tr_pred = model_k.predict(X_train).ravel()
        te_pred = model_k.predict(X_test).ravel()
        train_losses.append(mean_squared_error(Y_train, tr_pred))
        test_losses.append(mean_squared_error(Y_test,  te_pred))

    # Save per-fold loss curve
    plt.figure()
    plt.plot(train_losses, label="Train")
    plt.plot(test_losses,  label="Test")
    plt.title("Loss Curve")
    plt.xlabel("Epoch (n_components)")
    plt.ylabel("MSE")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"Model_Loss_{test_cultivar}.png")
    plt.close()

    per_fold_train_losses[test_cultivar] = train_losses
    per_fold_val_losses[test_cultivar]   = test_losses

    # === Evaluation on held-out cultivar with BEST model ===
    preds = best_model.predict(X_test).ravel()
    targets = Y_test.astype(float)

    # === Per-fold metrics ===
    mae  = mean_absolute_error(targets, preds)
    rmse = np.sqrt(mean_squared_error(targets, preds))
    r2   = safe_r2(targets, preds)  # <- avoid undefined R¬≤ for constant y

    print(f"\nPer-fold metrics ‚Äî {test_cultivar}:")
    print(f"MAE : {mae:.6f}")
    print(f"RMSE: {rmse:.6f}")
    print(f"R¬≤  : {'NA' if np.isnan(r2) else f'{r2:.6f}'}")
    print(f"Best n_components: {best_k}")

    per_fold_results.append({
        "cultivar": test_cultivar,
        "MAE": float(mae),
        "RMSE": float(rmse),
        "R2": (None if np.isnan(r2) else float(r2)),
        "Best_n_components": best_k,
        "Time_sec": float(time.time() - fold_start)
    })

    # Accumulate across folds for global summaries
    all_preds.extend(preds.tolist())
    all_targets.extend(targets.tolist())
    all_cultivars.extend([test_cultivar]*len(targets))

# =========================
# Global summaries (ALL folds)
# =========================
if len(all_preds) == 0:
    raise SystemExit("No predictions collected. Check CrossValidationData and filename prefixes.")

all_preds     = np.array(all_preds, dtype=float)
all_targets   = np.array(all_targets, dtype=float)
all_cultivars = np.array(all_cultivars, dtype=object)

# === Final model performance (global) ===
print("\nüìà Final Model Performance (Testing Set):")
mae  = mean_absolute_error(all_targets, all_preds)
rmse = np.sqrt(mean_squared_error(all_targets, all_preds))
r2   = r2_score(all_targets, all_preds)  # valid globally (non-constant)
print(f"MAE                 : {mae:.6f}")
print(f"RMSE                : {rmse:.6f}")
print(f"R¬≤                  : {r2:.6f}")
print(f"Max Error           : {max_error(all_targets, all_preds):{'.6f'}}")
print(f"MAPE                : {mean_absolute_percentage_error(all_targets, all_preds):.6f}")
print(f"Explained Variance  : {explained_variance_score(all_targets, all_preds):.6f}")

# === Per-spectrum table (same filename) ===
df = pd.DataFrame({"True Cys": all_targets, "Predicted Cys": all_preds})
# robust mapping from true value to cultivar via rounding (handles float jitter)
inv_map = {round(float(v), 6): k for k, v in cultivar_cys_map.items()}
df["Cultivar"] = [inv_map.get(round(float(t), 6), "Unknown") for t in df["True Cys"].values]
df.to_csv("test_predictions.csv", index=False)

# === Separate Error Bars (Model vs Measurement) ‚Äî per-spectrum ===
unique_cvs_order = list(df["Cultivar"].unique())
palette20 = [plt.get_cmap("tab20")(i) for i in range(20)]
color_map = {cv: palette20[i % 20] for i, cv in enumerate(unique_cvs_order)}

plt.figure(figsize=(8, 6))
seen = set()
for cultivar in unique_cvs_order:
    sub = df[df["Cultivar"] == cultivar]
    y_true = sub["True Cys"].to_numpy()
    y_pred = sub["Predicted Cys"].to_numpy()

    model_error = np.abs(y_pred - y_true)
    # meas_error not plotted; kept for parity
    _meas_error  = float(cultivar_sd_map.get(cultivar, 0.0))

    label = cultivar if cultivar not in seen else "_nolegend_"
    seen.add(cultivar)

    plt.errorbar(
        y_true, y_pred, yerr=model_error,
        fmt='o', label=label, alpha=0.8, capsize=3, elinewidth=1.2,
        color=color_map[cultivar]
    )
    plt.errorbar(
        y_true, y_pred,
        fmt='none', ecolor='gray', linestyle='dotted',
        capsize=2, elinewidth=1.0
    )

mn = float(min(df["True Cys"].min(), df["Predicted Cys"].min()))
mx = float(max(df["True Cys"].max(), df["Predicted Cys"].max()))
plt.plot([mn, mx], [mn, mx], 'k--', label="_nolegend_")
plt.title("Predicted vs. True with Separate Error Bars (Model vs. Measurement)")
plt.xlabel("True Cys (g/100g)")
plt.ylabel("Predicted Cys (g/100g)")
plt.grid(True, alpha=0.2)
plt.legend(loc="upper left", bbox_to_anchor=(1.02, 1.0), borderaxespad=0.0, frameon=True)
plt.tight_layout()
plt.savefig("Predicted_vs_True_Separate_Errors.png")
plt.close()

# === Separate Error Bars (MEAN per cultivar) ===
g = (df.groupby("Cultivar")
       .agg(True_Cys=("True Cys", "first"),
            Mean_Pred=("Predicted Cys", "mean"),
            Std_Pred=("Predicted Cys", "std"),
            N=("Predicted Cys", "size"))
       .reset_index())

plt.figure(figsize=(10, 6))
for _, row in g.iterrows():
    cultivar = row["Cultivar"]
    x = float(row["True_Cys"])
    y = float(row["Mean_Pred"])
    yerr = float(row["Std_Pred"] / np.sqrt(row["N"]) if USE_STANDARD_ERROR else row["Std_Pred"])
    # xerr available as cultivar SD if desired
    _xerr = float(cultivar_sd_map.get(cultivar, 0.0))
    c = color_map.get(cultivar, palette20[0])

    plt.errorbar(
        x, y, yerr=yerr, fmt='o', capsize=3, elinewidth=1.2,
        label=cultivar, alpha=0.9, color=c
    )
    plt.errorbar(
        x, y, fmt='none', ecolor='gray',
        linestyle='dotted', capsize=2, elinewidth=1.0
    )

mn = float(min(g["True_Cys"].min(), g["Mean_Pred"].min()))
mx = float(max(g["True_Cys"].max(), g["Mean_Pred"].max()))
ax = plt.gca()
ax.set_aspect('equal', adjustable='box')
pad = 0.002
ax.set_xlim(mn - pad, mx + pad)
ax.set_ylim(mn - pad, mx + pad)
ax.plot([mn - pad, mx + pad], [mn - pad, mx + pad], 'k--', label='Ideal (y=x)')

X_means = g["True_Cys"].to_numpy().reshape(-1, 1)
y_means = g["Mean_Pred"].to_numpy()
reg = LinearRegression().fit(X_means, y_means)
slope = float(reg.coef_[0]); intercept = float(reg.intercept_)
x_line = np.linspace(mn - pad, mx + pad, 200)
ax.plot(x_line, slope * x_line + intercept, '-', linewidth=2, color='k',
        label=f'Fit: y={slope:.3f}x+{intercept:.3f}')

r2_means = r2_score(g["True_Cys"], g["Mean_Pred"])  # on 20 points (valid)
r2_fit   = r2_score(y_means, reg.predict(X_means))

plt.title("Mean Predicted vs True HPLC per Cultivar")
plt.xlabel("True HPLC Cys (g/100g)")
plt.ylabel("Mean Predicted Cys (g/100g)")
ax.legend(loc="upper left", bbox_to_anchor=(1.02, 1.0), borderaxespad=0.0, frameon=True)
plt.tight_layout(rect=(0.0, 0.0, 0.80, 1.0))
plt.savefig("Predicted_vs_True_Separate_Errors_MEAN.png")
plt.close()

# --- Mean-level metrics + bar chart ---
mae_means  = mean_absolute_error(g["True_Cys"], g["Mean_Pred"])
rmse_means = np.sqrt(mean_squared_error(g["True_Cys"], g["Mean_Pred"]))
print("\nPer-cultivar mean metrics:")
print(f"R¬≤(means) : {r2_means:.6f}")
print(f"MAE(means): {mae_means:.6f}")
print(f"RMSE(means): {rmse_means:.6f}")
print(f"Slope(means): {slope:.3f} | Intercept(means): {intercept:.3f}")
print(f"Std(pred)/Std(true) on means: {y_means.std(ddof=1) / X_means.std(ddof=1):.3f}")

# Report R¬≤(means) ¬± SD(per-fold R¬≤) ignoring undefined per-fold R¬≤
if len(per_fold_results) > 0:
    r2_values = np.array([res["R2"] for res in per_fold_results if res["R2"] is not None], dtype=float)
    r2_sd = float(np.std(r2_values, ddof=1)) if len(r2_values) > 1 else 0.0
    print(f"R¬≤(means) ¬± SD(per-fold R¬≤): {r2_means:.6f} ¬± {r2_sd:.6f}")

print("\nNormalized error (per-spectrum):")
y_std   = all_targets.std(ddof=1)
y_range = all_targets.max() - all_targets.min()
rmse_all = np.sqrt(mean_squared_error(all_targets, all_preds))
print(f"RMSE / std(y):   {rmse_all / y_std:.3f}")
print(f"RMSE / range(y): {rmse_all / y_range:.3f}")

# --- Bar chart of mean vs true with error bars ---
SHOW_HPLC_SD = False
yerr_pred = (g["Std_Pred"] / np.sqrt(g["N"])) if USE_STANDARD_ERROR else g["Std_Pred"]
yerr_pred = np.nan_to_num(yerr_pred.to_numpy(), nan=0.0)
yerr_true = np.array([cultivar_sd_map.get(cv, 0.0) for cv in g["Cultivar"]]) if SHOW_HPLC_SD else None

plt.figure(figsize=(10, 6))
x_pos = np.arange(len(g))
bar_w = 0.38
plt.bar(x_pos - bar_w/2, g["True_Cys"], width=bar_w, label="True HPLC",
        yerr=yerr_true, capsize=3 if SHOW_HPLC_SD else 0)
plt.bar(x_pos + bar_w/2, g["Mean_Pred"], width=bar_w,
        label=f"Mean Predicted",
        yerr=yerr_pred, capsize=3)
plt.xticks(x_pos, g["Cultivar"], rotation=45, ha="right")
plt.ylabel("Cys Concentration (g/100g)")
plt.title("Mean Predicted vs True HPLC values per Cultivar")
plt.grid(axis='y', alpha=0.2)
plt.legend()
plt.tight_layout()
plt.savefig("bar_mean_true_vs_pred.png")
plt.close()

# Save per-cultivar table with absolute errors
g_out = g.copy()
g_out["Abs_Error"]    = (g_out["Mean_Pred"] - g_out["True_Cys"]).abs()
g_out["Pct_Error_%"]  = 100.0 * g_out["Abs_Error"] / g_out["True_Cys"]
g_out["SE_Pred"]      = g_out["Std_Pred"] / np.sqrt(g_out["N"])
cols = ["Cultivar", "True_Cys", "Mean_Pred", "Abs_Error", "Pct_Error_%", "N", "Std_Pred", "SE_Pred"]
g_out[cols].to_csv("mean_vs_true_by_cultivar.csv", index=False)

print("\nSaved bar plot -> bar_mean_true_vs_pred.png")
print("Saved per-cultivar summary -> mean_vs_true_by_cultivar.csv")

# === Predicted vs True scatter for the (global) test set ===
plt.figure(figsize=(6, 6))
plt.scatter(all_targets, all_preds, alpha=0.7)
mn, mx = min(all_targets.min(), all_preds.min()), max(all_targets.max(), all_preds.max())
plt.plot([mn, mx], [mn, mx], 'k--', label='Ideal')
plt.xlabel("True Cys (g/100g)")
plt.ylabel("Predicted Cys (g/100g)")
plt.title("Predicted vs True Cys (Testing Set)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("calibration_plot_testing_set.png")
plt.close()

# === Aggregate "Model_Loss.png" across folds (average per-epoch/components) ===
if len(per_fold_train_losses) > 0:
    # Folds may have different max_components; pad with NaN and average ignoring NaN
    max_len = max(len(v) for v in per_fold_train_losses.values())
    tmat = np.full((len(per_fold_train_losses), max_len), np.nan, dtype=float)
    vmat = np.full((len(per_fold_val_losses),   max_len), np.nan, dtype=float)
    for i, cvk in enumerate(per_fold_train_losses):
        tl = per_fold_train_losses[cvk]
        vl = per_fold_val_losses[cvk]
        tmat[i, :len(tl)] = tl
        vmat[i, :len(vl)] = vl
    mean_train = np.nanmean(tmat, axis=0)
    mean_val   = np.nanmean(vmat, axis=0)

    plt.figure()
    plt.plot(mean_train, label="Train")
    plt.plot(mean_val,   label="Test")
    plt.title("Loss Curve")
    plt.xlabel("Epoch (n_components)")
    plt.ylabel("MSE")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig("Model_Loss.png")
    plt.close()

# Save per-fold metrics (extra)
pd.DataFrame(per_fold_results).to_csv("per_fold_metrics.csv", index=False)

print(f"\n‚è±Ô∏è Total run time: {time.time() - start_time:.2f} seconds")
